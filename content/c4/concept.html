

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Concept &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/mystnb.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}})</script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Construction" href="construction.html" />
    <link rel="prev" title="Code" href="../c3/code.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../table_of_contents.html">Table of Contents</a>
  </li>
  <li class="">
    <a href="../conventions_notation.html">Conventions and Notation</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">1. Ordinary Linear Regression</p>
</li>
  <li class="">
    <a href="../c1/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c1/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c1/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">2. Linear Regression Extensions</p>
</li>
  <li class="">
    <a href="../c2/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c2/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c2/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">3. Discriminative Classifiers (Logistic Regression)</p>
</li>
  <li class="">
    <a href="../c3/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c3/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c3/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">4. Generative Classifiers (Naive Bayes)</p>
</li>
  <li class="active">
    <a href="">Concept</a>
  </li>
  <li class="">
    <a href="construction.html">Construction</a>
  </li>
  <li class="">
    <a href="code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">5. Decision Trees</p>
</li>
  <li class="">
    <a href="../c5/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c5/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c5/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">6. Tree Ensemble Methods</p>
</li>
  <li class="">
    <a href="../c6/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c6/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c6/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">7. Neural Networks</p>
</li>
  <li class="">
    <a href="../c7/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../c7/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../c7/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Appendix</p>
</li>
  <li class="">
    <a href="../appendix/math.html">Math</a>
  </li>
  <li class="">
    <a href="../appendix/probability.html">Probability</a>
  </li>
  <li class="">
    <a href="../appendix/methods.html">Common Methods</a>
  </li>
  <li class="">
    <a href="../appendix/data.html">Datasets</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../../_sources/content/c4/concept.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        <a class="edit-button" href="https://github.com/dafriedman97/book/edit/master/content/c4/concept.md"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" title="Edit this page"><i class="fas fa-pencil-alt"></i></button></a>

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#model-structure" class="nav-link">1. Model Structure</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#parameter-estimation" class="nav-link">2. Parameter Estimation</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#class-priors" class="nav-link">2.1 Class Priors</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#data-likelihood" class="nav-link">2.2 Data Likelihood</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h4">
            <a href="#linear-discriminative-analysis-lda" class="nav-link">2.2.1 Linear Discriminative Analysis (LDA)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h4">
            <a href="#quadratic-discriminative-analysis-qda" class="nav-link">2.2.2 Quadratic Discriminative Analysis (QDA)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h4">
            <a href="#naive-bayes" class="nav-link">2.2.3 Naive Bayes</a>
        </li>
    
            </ul>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#making-classifications" class="nav-link">3. Making Classifications</a>
        </li>
    
    </ul>
</nav>



<div class="tocsection editthispage">
    <a href="https://github.com/dafriedman97/book/edit/master/content/c4/concept.md">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\sumK}{\sum_{k = 1}^K}
\newcommand{\sumk}{\sum_k}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\prodK}{\prod_{k = 1}^K}
\newcommand{\by}{\mathbf{y}} 
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\newcommand{\collection}{\{\bx_n, y_n\}_{n = 1}^N}
\newcommand{\l}{\left(}
\newcommand{\r}{\right)}
\]</div>
<p>Discriminative classifiers, as we saw in the previous chapter, model a target variable as a direct function of one or more predictors. Generative classifiers, the subject of this chapter, instead view the predictors as being generated according to their class—i.e., they see the predictors as a function of the target, rather than the other way around. They then use Bayes’ rule to turn <span class="math notranslate nohighlight">\(P(\bx_n|Y_n = k)\)</span> into <span class="math notranslate nohighlight">\(P(Y_n = k|\bx_n)\)</span>.</p>
<p>In generative classifiers, we view both the target and the predictors as random variables. We will therefore refer to the target variable with <span class="math notranslate nohighlight">\(Y_n\)</span>, but in order to avoid confusing it with a matrix, we refer to the predictor vector with <span class="math notranslate nohighlight">\(\bx_n\)</span>.</p>
<p>Generative models can be broken down into the three following steps. Suppose we have a classification task with <span class="math notranslate nohighlight">\(K\)</span> unordered classes, represented by <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>.</p>
<ol class="simple">
<li><p>Estimate the density of the predictors conditional on the target belonging to each class. I.e., estimate <span class="math notranslate nohighlight">\(p(\bx_n|Y_n = k)\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>.</p></li>
<li><p>Estimate the prior probability that a target belongs to any given class. I.e., estimate <span class="math notranslate nohighlight">\(P(Y_n = k)\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>. This is also written as <span class="math notranslate nohighlight">\(p(Y_n)\)</span>.</p></li>
<li><p>Using Bayes’ rule, calculate the posterior probability that the target belongs to any given class. I.e., calculate  <span class="math notranslate nohighlight">\(p(Y_n = k|\bx_n) \propto p(\bx_n|Y_n = k)p(Y_n = k)\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>.</p></li>
</ol>
<p>We then classify observation <span class="math notranslate nohighlight">\(n\)</span> as being from the class for which <span class="math notranslate nohighlight">\(P(Y_n = k|\bx_n)\)</span> is greatest. In math,</p>
<div class="math notranslate nohighlight">
\[
\hat{Y}_n = \underset{k}{\text{arg max }} p(Y_n = k|\bx_n).
\]</div>
<p>Note that we do not need <span class="math notranslate nohighlight">\(p(\bx_n)\)</span>, which would be the denominator in the Bayes’ rule formula, since it would be equal across classes.</p>
<div class="alert alert-info">
<p class="admonition-title">Note</p>
<p>This chapter is oriented differently from the others. The main methods discussed—Linear Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes—share much of the same structure. Rather than introducing each individually, we describe them together and note (in section 2.2) how they differ.</p>
</div>
<div class="section" id="model-structure">
<h2>1. Model Structure<a class="headerlink" href="#model-structure" title="Permalink to this headline">¶</a></h2>
<p>A generative classifier models two sources of randomness. First, we assume that out of the <span class="math notranslate nohighlight">\(K\)</span> possible classes, each observation belongs to class <span class="math notranslate nohighlight">\(k\)</span> independently with probability <span class="math notranslate nohighlight">\(\pi_k\)</span>. In other words, letting <span class="math notranslate nohighlight">\(\bpi =\begin{bmatrix} \pi_1 &amp; \dots &amp; \pi_K\end{bmatrix}^\top \in \mathbb{R}^{K}\)</span>, we assume the prior</p>
<div class="math notranslate nohighlight">
\[
y_n \iid \text{Cat}(\bpi).
\]</div>
<p>See the math note below on the Categorical distribution.</p>
<div class="admonition-math-note alert alert-info">
<p class="admonition-title">Math Note</p>
<p>A random variable which takes on one of <span class="math notranslate nohighlight">\(K\)</span> discrete and unordered outcomes with probabilities <span class="math notranslate nohighlight">\(\pi_1, \dots, \pi_K\)</span> follows the Categorical distribution with parameter <span class="math notranslate nohighlight">\(\bpi = \begin{bmatrix} \pi_1 &amp; \dots &amp; \pi_K \end{bmatrix}^\top\)</span>, written <span class="math notranslate nohighlight">\(\text{Cat}(\bpi)\)</span>. For instance, a single die roll is distributed <span class="math notranslate nohighlight">\(\text{Cat}(\bpi)\)</span> for <span class="math notranslate nohighlight">\(\bpi = \begin{bmatrix} 1/6 \dots 1/6 \end{bmatrix}^\top\)</span>.</p>
<p>The density for <span class="math notranslate nohighlight">\(Y \sim \text{Cat}(\bp)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(Y = 1) &amp;= p_1 \\
&amp;... \\
P(Y = K) &amp;= p_K.
\end{align*}
\end{split}\]</div>
<p>This can be written more compactly as</p>
<div class="math notranslate nohighlight">
\[
p(y) = \prod_{k = 1}^K p_k ^{I_k}
\]</div>
<p>where <span class="math notranslate nohighlight">\(I_k\)</span> is an indicator that equals 1 if <span class="math notranslate nohighlight">\(y = k\)</span> and 0 otherwise.</p>
</div>
<p>We then assume some distribution for <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> conditional on observation <span class="math notranslate nohighlight">\(n\)</span>’s class, <span class="math notranslate nohighlight">\(Y_n\)</span>. We typically assume all the <span class="math notranslate nohighlight">\(\bx_n\)</span> come from the same <em>family</em> of distributions, though the parameters depend on their class. For instance, we might have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\bx_n|(Y_n = 1) &amp;\sim \text{MVN}(\bmu_1, \bSigma_1), \\ &amp;... \\
\bx_{n}|(Y_n = K)  &amp;\sim \text{MVN}(\bmu_K, \bSigma_K),
\end{align*}
\end{split}\]</div>
<p>though we wouldn’t let one conditional distribution be Multivariate Normal and another be Multivariate <span class="math notranslate nohighlight">\(t\)</span>. Note that it is possible, however, for the individual variables within the random vector <span class="math notranslate nohighlight">\(\bx_n\)</span> to follow different distributions. For instance, if <span class="math notranslate nohighlight">\(\bx_n = \begin{bmatrix} x_{n1} &amp; x_{n2} \end{bmatrix}^\top\)</span>, we might have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
x_{n1}|(Y_n = k) &amp;\sim \text{Bin}(n, p_k)
\\
x_{n2}|(Y_n = k) &amp;\sim \mathcal{N}(\bmu_k, \bSigma_k) 
\end{align*}
\end{split}\]</div>
<p>The machine learning task is to estimate the parameters of these models—<span class="math notranslate nohighlight">\(\bpi\)</span> for <span class="math notranslate nohighlight">\(Y_n\)</span> and whatever parameters might index the possible distributions of <span class="math notranslate nohighlight">\(\bx_n|Y_n\)</span>, in this case <span class="math notranslate nohighlight">\(\bmu_k\)</span> and <span class="math notranslate nohighlight">\(\bSigma_k\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>. Once that’s done, we can estimate <span class="math notranslate nohighlight">\(p(Y_n = k)\)</span> and <span class="math notranslate nohighlight">\(p(\bx_n|Y_n = k)\)</span> for each class and, through Bayes’ rule, choose the class that maximizes <span class="math notranslate nohighlight">\(p(Y_n = k|\bx_n)\)</span>.</p>
</div>
<div class="section" id="parameter-estimation">
<h2>2. Parameter Estimation<a class="headerlink" href="#parameter-estimation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="class-priors">
<h3>2.1 Class Priors<a class="headerlink" href="#class-priors" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by deriving the estimates for <span class="math notranslate nohighlight">\(\bpi\)</span>, the class priors. Let <span class="math notranslate nohighlight">\(I_{nk}\)</span> be an indicator which equals 1 if <span class="math notranslate nohighlight">\(Y_n = k\)</span> and 0 otherwise. Then the joint likelihood and log-likelihood are given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right) &amp;= \prodN \prod_{k = 1}^K \pi_k^{I_{nk}} 
\\
\log L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right) &amp;= \sumN \sum_{k = 1}^K I_{nk} \log(\pi_k)
\\
&amp;= \sum_{k = 1}^K N_k\log(\pi_k),
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_k = \sumN I_{nk}\)</span> gives the number of observations in class <span class="math notranslate nohighlight">\(k\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>.</p>
<div class="admonition-math-note alert alert-info">
<p class="admonition-title">Math Note</p>
<p>The <em>Lagrangian function</em> provides a method for optimizing a function <span class="math notranslate nohighlight">\(f(\bx)\)</span> subject to the constraint <span class="math notranslate nohighlight">\(g(\bx) = 0\)</span>.  The Lagrangian is given by</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx).
\]</div>
<p><span class="math notranslate nohighlight">\(\lambda\)</span> is known as the <em>Lagrange multiplier</em>. The critical points of <span class="math notranslate nohighlight">\(f(\bx)\)</span> (subject to the equality constraint) are found by setting the gradients of <span class="math notranslate nohighlight">\(\mathcal{L}(\lambda, \bx)\)</span> with respect to <span class="math notranslate nohighlight">\(\lambda\)</span> and <span class="math notranslate nohighlight">\(\bx\)</span> equal to 0.</p>
</div>
<p>Noting the constraint <span class="math notranslate nohighlight">\(\sum_{k = 1}^K \pi_k = 1\)</span> (or equivalently <span class="math notranslate nohighlight">\(\sum_{k = 1}^K\pi_k - 1 = 0\)</span>), we can maximize the log-likelihood with the following Lagrangian.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L}(\bpi) &amp;= \sum_{k = 1}^K N_k \log(\pi_k) - \lambda(\sum_{k = 1}^K \pi_k - 1).
\\
\dadb{\mathcal{L}(\bpi)}{\pi_k} &amp;= \frac{N_k}{\pi_k} - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1, \dots, K\}
\\
\dadb{\mathcal{L}(\bpi)}{\lambda} &amp;= 1 - \sum_{k = 1}^K \pi_k.
\\
\end{align*}
\end{split}\]</div>
<p>This system of equations gives an intuitive solution:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N,
\]</div>
<p>which says that our estimate of <span class="math notranslate nohighlight">\(p(Y_n = k)\)</span> is just the sample fraction of observations from class <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
<div class="section" id="data-likelihood">
<h3>2.2 Data Likelihood<a class="headerlink" href="#data-likelihood" title="Permalink to this headline">¶</a></h3>
<p>The next step is to model the conditional distribution of <span class="math notranslate nohighlight">\(\bx_n\)</span> given <span class="math notranslate nohighlight">\(Y_n\)</span> so that we can estimate this distribution’s parameters. This of course depends on the family of distributions we choose to model <span class="math notranslate nohighlight">\(\bx_n\)</span>. Three common approaches are detailed below.</p>
<div class="section" id="linear-discriminative-analysis-lda">
<h4>2.2.1 Linear Discriminative Analysis (LDA)<a class="headerlink" href="#linear-discriminative-analysis-lda" title="Permalink to this headline">¶</a></h4>
<p>In LDA, we assume</p>
<div class="math notranslate nohighlight">
\[
\bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma),
\]</div>
<p>for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>. Note that each class has the same covariance matrix but a unique mean vector.</p>
<p>Let’s derive the parameters in this case. First, let’s find the likelihood and log-likelihood. Note that we can write the joint likelihood as follows,</p>
<div class="math notranslate nohighlight">
\[
L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k, \bSigma\r\Big)^{I_{nk}},
\]</div>
<p>since <span class="math notranslate nohighlight">\(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\)</span> equals 1 if <span class="math notranslate nohighlight">\(y_n \neq k\)</span> and <span class="math notranslate nohighlight">\(p(\bx_n|\bmu_k, \bSigma)\)</span> otherwise. Then we plug in the Multivariate Normal PDF (dropping multiplicative constants) and take the log, as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &amp;= \prodN\prodK \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k)\right\}\Big)^{I_{nk}} 
\\
\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &amp;= \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k) \r
\end{align*}
\end{split}\]</div>
<div class="admonition-math-note alert alert-info">
<p class="admonition-title">Math Note</p>
<p>The following matrix derivatives will be of use for maximizing the above log-likelihood.</p>
<p>For any invertible matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\)</span>. It follows that</p>
<div class="math notranslate nohighlight">
\[
\dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2}
\]</div>
<p>We also have</p>
<div class="math notranslate nohighlight">
\[
\dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx \bx^\top \mathbf{W}^{-\top}. \tag{3}
\]</div>
<p>For any symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}} = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4}
\]</div>
<p>These results come from the <a class="reference external" href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix Cookbook</a>.</p>
</div>
<p>Let’s start by estimating <span class="math notranslate nohighlight">\(\bSigma\)</span>. First, simplify the log-likelihood to make the gradient with respect to <span class="math notranslate nohighlight">\(\bSigma\)</span> more apparent.</p>
<div class="math notranslate nohighlight">
\[
\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}  \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k).
\]</div>
<p>Then, using equations (2) and (3) from the <em>Math Note</em>, we get</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &amp;= -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}.
\end{align*}
\]</div>
<p>Finally, we set this equal to 0 and multiply by <span class="math notranslate nohighlight">\(\bSigma^{-1}\)</span> on the left to solve for <span class="math notranslate nohighlight">\(\hat{\bSigma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
0 &amp;= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\
\bSigma^\top &amp;= \frac{1}{N}\sumN \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\
\hat{\bSigma} &amp;= \frac{1}{N}\mathbf{S}_T,
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\)</span>.</p>
<p>Now, to estimate the <span class="math notranslate nohighlight">\(\bmu_k\)</span>, let’s look at each class individually. Let <span class="math notranslate nohighlight">\(N_k\)</span> be the number of observations in class <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(C_k\)</span> be the set of observations in class <span class="math notranslate nohighlight">\(k\)</span>. Looking only at terms involving <span class="math notranslate nohighlight">\(\bmu_k\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log L(\bmu_k, \bSigma) &amp;= -\frac{1}{2} \sum_{n \in C_k} \Big( \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big).
\end{align*}
\]</div>
<p>Using equation (4) from the <em>Math Note</em>, we calculate the gradient as</p>
<div class="math notranslate nohighlight">
\[
\dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n - \bmu_k).
\]</div>
<p>Setting this gradient equal to 0 and solving, we obtain our <span class="math notranslate nohighlight">\(\bmu_k\)</span> estimate:</p>
<div class="math notranslate nohighlight">
\[
\hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\bx}_k\)</span> is the element-wise sample mean of all <span class="math notranslate nohighlight">\(\bx_n\)</span> in class <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
<div class="section" id="quadratic-discriminative-analysis-qda">
<h4>2.2.2 Quadratic Discriminative Analysis (QDA)<a class="headerlink" href="#quadratic-discriminative-analysis-qda" title="Permalink to this headline">¶</a></h4>
<p>QDA looks very similar to LDA but assumes each class has its <em>own</em> covariance matrix:</p>
<div class="math notranslate nohighlight">
\[
\bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k)
\]</div>
<p>for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>. The log-likelihood is the same as in LDA except we replace <span class="math notranslate nohighlight">\(\bSigma\)</span> with <span class="math notranslate nohighlight">\(\bSigma_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &amp;= \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k) \r.
\end{align*}
\]</div>
<p>Again, let’s look at the parameters for each class individually. The log-likelihood for class <span class="math notranslate nohighlight">\(k\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log L(\bmu_k, \bSigma_k) &amp;= -\frac{1}{2} \sum_{n \in C_k} \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big). 
\end{align*}
\]</div>
<p>We could take the gradient of this log-likelihood with respect to <span class="math notranslate nohighlight">\(\bmu_k\)</span> and set it equal to 0 to solve for <span class="math notranslate nohighlight">\(\hat{\bmu}_k\)</span>. However, we can also note that our <span class="math notranslate nohighlight">\(\hat{\bmu}_k\)</span> estimate from the LDA approach will hold since this expression didn’t depend on the covariance term (which is the only thing we’ve changed). Therefore, we again get</p>
<div class="math notranslate nohighlight">
\[
\hat{\bmu}_k = \bar{\bx}_k.
\]</div>
<p>To estimate the <span class="math notranslate nohighlight">\(\bSigma_k\)</span>, we take the gradient of the log-likelihood for class <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &amp;= -\frac{1}{2}\sum_{n \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top} \right).
\end{align*}
\]</div>
<p>Then we set this equal to 0 to solve for <span class="math notranslate nohighlight">\(\hat{\bSigma}_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{n \in C_k} \bSigma_k^{-\top}  &amp;= \sum_{n \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}  \\
N_k I &amp;= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top} \\ 
\bSigma_k^\top  &amp;= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\
\hat{\bSigma}_k &amp;= \frac{1}{N_k} \mathbf{S}_k,
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\)</span>.</p>
</div>
<div class="section" id="naive-bayes">
<h4>2.2.3 Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h4>
<p>Naive Bayes assumes the random variables within <span class="math notranslate nohighlight">\(\bx_n\)</span> are independent conditional on the class of observation <span class="math notranslate nohighlight">\(n\)</span>. I.e. if <span class="math notranslate nohighlight">\(\bx_n \in \mathbb{R}^D\)</span>, Naive Bayes assumes</p>
<div class="math notranslate nohighlight">
\[
p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
\]</div>
<p>This makes estimating <span class="math notranslate nohighlight">\(p(\bx_n|Y_n)\)</span> very easy—to estimate the parameters of <span class="math notranslate nohighlight">\(p(x_{nd}|Y_n)\)</span>, we can ignore all the variables in <span class="math notranslate nohighlight">\(\bx_{n}\)</span> other than <span class="math notranslate nohighlight">\(x_{nd}\)</span>!</p>
<p>As an example, assume <span class="math notranslate nohighlight">\(\bx_n \in \mathbb{R}^2\)</span> and we use the following model (where for simplicity <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(\sigma^2_k\)</span> are known).</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
x_{n1}|(Y_n = k) &amp;\sim \mathcal{N}(\mu_k, \sigma^2_k) \\
x_{n2}|(Y_n = k) &amp;\sim \text{Bin}(n, p_k).
\end{align*}
\end{split}\]</div>
<p>Let the <span class="math notranslate nohighlight">\(\btheta_k = (\mu_k, \sigma_k^2, p_k)\)</span> contain all the parameters for class <span class="math notranslate nohighlight">\(k\)</span> . The joint likelihood function would become</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L(\{\btheta_k\}_{k = 1}^K) &amp;= \prodN \prodK \l p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\
L(\{\btheta_k\}_{k = 1}^K) &amp;= \prodN \prodK \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}},
\end{align*}
\end{split}\]</div>
<p>where the two are equal because of the Naive Bayes conditional independence assumption. This allows us to easily find maximum likelihood estimates. The rest of this sub-section demonstrates how those estimates would be found, though it is nothing beyond ordinary maximum likelihood estimation.</p>
<p>The log-likelihood is given by</p>
<div class="math notranslate nohighlight">
\[
\log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k, \sigma^2_k) + \log p(x_{n2}|p_k) \r.
\]</div>
<p>As before, we estimate the parameters in each class by looking only at the terms in that class. Let’s look at the log-likelihood for class <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log L(\btheta_k) &amp;= \sum_{n \in C_k} \log p(x_{n1}|\mu_k, \sigma^2_k) + \log p(x_{n2}|p_k) \\ 
&amp;= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k} + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k).
\end{align*}
\end{split}\]</div>
<p>Taking the derivative with respect to <span class="math notranslate nohighlight">\(p_k\)</span>, we’re left with</p>
<div class="math notranslate nohighlight">
\[
\dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
\]</div>
<p>which, will give us <span class="math notranslate nohighlight">\(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\)</span> as usual. The same process would again give typical results for <span class="math notranslate nohighlight">\(\mu_k\)</span> and and <span class="math notranslate nohighlight">\(\sigma^2_k\)</span>.</p>
</div>
</div>
</div>
<div class="section" id="making-classifications">
<h2>3. Making Classifications<a class="headerlink" href="#making-classifications" title="Permalink to this headline">¶</a></h2>
<p>Regardless of our modeling choices for <span class="math notranslate nohighlight">\(p(\bx_n|Y_n)\)</span>, classifying new observations is easy. Consider a test observation <span class="math notranslate nohighlight">\(\bx_0\)</span>. For <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>, we use Bayes’ rule to calculate</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(Y_0 = k|\bx_0) &amp;\propto p(\bx_0|Y_0 = k)p(Y_0 = k) 
\\
&amp;= \hat{p}(\bx_0|Y_0 = k)\hat{\pi}_k,
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{p}\)</span> gives the estimated density of <span class="math notranslate nohighlight">\(\bx_0\)</span> conditional on <span class="math notranslate nohighlight">\(Y_0\)</span>. We then predict <span class="math notranslate nohighlight">\(Y_0 = k\)</span> for whichever value <span class="math notranslate nohighlight">\(k\)</span> maximizes the above expression.</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../c3/code.html" title="previous page">Code</a>
    <a class='right-next' id="next-link" href="construction.html" title="next page">Construction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>