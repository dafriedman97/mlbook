

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Boosting &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Construction" href="../construction.html" />
    <link rel="prev" title="Random Forests" href="random_forests.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../table_of_contents.html">
   Table of Contents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../conventions_notation.html">
   Conventions and Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  1. Ordinary Linear Regression
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c1/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c1/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c1/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  2. Linear Regression Extensions
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c2/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c2/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c2/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  3. Discriminative Classifiers (Logistic Regression)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c3/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c3/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c3/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  4. Generative Classifiers (Naive Bayes)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c4/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c4/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c4/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  5. Decision Trees
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c5/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c5/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c5/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  6. Tree Ensemble Methods
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="../concept.html">
   Concept
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="bagging.html">
     Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Boosting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  7. Neural Networks
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c7/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c7/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c7/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/math.html">
   Math
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/probability.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/methods.html">
   Common Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/data.html">
   Datasets
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/c6/s1/boosting.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dafriedman97/mlbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dafriedman97/mlbook/issues/new?title=Issue%20on%20page%20%2Fcontent/c6/s1/boosting.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/dafriedman97/mlbook/edit/master/content/c6/s1/boosting.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaboost-for-binary-classification">
   AdaBoost for Binary Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted-classification-trees">
     Weighted Classification Trees
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-discrete-adaboost-algorithm">
     The Discrete AdaBoost Algorithm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaboost-for-regression">
   AdaBoost For Regression
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="boosting">
<h1>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\]</div>
<p>The common sense of boosting was beautifully introduced in the <a href="https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">A Short Introduction to Boosting</a> paper on AdaBoost. It states that, suppose a horse-racing gambler, hoping to maximize his winnings, decides to create a computer program that will accurately predict the winner of a horse race based on the usual information(number of races recently won by each horse, betting odds for each horse, etc.). To create such a program, he asks a highly successful gambler to explain his betting strategy. Not surprisingly, the expert is unable to articulate a grand set of rules for selecting a horse. On the other hand, when presented with the data for a specific set of races,the expert has no trouble coming up with a rule of thumb for that set of races(such as, bet on the horse that has recently won the most races or bet on the horse with the most favored odds). Although such a rule of thumb, by itself, is obviously very rough and inaccurate, it is not unreasonable to expect it to provide predictions that are atleast a little bit better than random guessing. Further more, by repeatedly asking the expert's opinion on different collections of races, the gambler is able to extract many rules of thumb. In order to use these rules of thumb to maximum advantage, there are two problems faced by the gambler: First, how should he choose the collections of races presented to the expert so as to extract rules of thumb from the expert that will be the most useful? Second, once he has collected many rules of thumb, how can they be combined into a single, highly accurate prediction rule? Boosting refers to a general and provably effective method of producing a very accurate pre-diction rule by combining rough and moderately inaccurate rules of thumb in a manner similar to that suggested above.</p>
<p>Like bagging and random forests, boosting combines multiple weak learners into one improved model. Unlike bagging and random forests, however, boosting trains these weak learners <em>sequentially</em>, each one learning from the mistakes of the last. Rather than a single model, “boosting” refers to a class of sequential learning methods.</p>
<p>Here we discuss two specific algorithms for conducting boosting. The first, known as <em>Discrete AdaBoost</em> (or just <em>AdaBoost</em>), is used for binary classification. The second, known as <em>AdaBoost.R2</em>, is used for regression.</p>
<div class="section" id="adaboost-for-binary-classification">
<h2>AdaBoost for Binary Classification<a class="headerlink" href="#adaboost-for-binary-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="weighted-classification-trees">
<h3>Weighted Classification Trees<a class="headerlink" href="#weighted-classification-trees" title="Permalink to this headline">¶</a></h3>
<p>Weak learners in a boosting model learn from the mistakes of previous iterations by increasing the <em>weights</em> of observations that previous learners struggled with. How exactly we fit a <em>weighted</em> learner depends on the type of learner. Fortunately, for classification trees this can be done with just a slight adjustment to the loss function.</p>
<p>Consider a  classification tree with classes <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span> and a node <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span>. Let  <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> give the fraction of the observations in <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> belonging to class <span class="math notranslate nohighlight">\(k\)</span>. Recall that the Gini index is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{G}(\mathcal{N}_m) = \sum_{k = 1}^K \hat{p}_{mk}(1-\hat{p}_{mk}),
\]</div>
<p>and the cross entropy as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{E}(\mathcal{N}_m) = -\sum_{k = 1}^K \hat{p}_{mk} \log\hat{p}_{mk},
\]</div>
<p>The classification tree then chooses the split <span class="math notranslate nohighlight">\(S_m\)</span> that minimizes the combined loss of the child nodes,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(S_m) = f_L\cdot \mathcal{L}(\mathcal{C}_m^L) +  f_R\cdot \mathcal{L}(\mathcal{C}_m^R),
\]</div>
<p>where <span class="math notranslate nohighlight">\(C_m^L\)</span> and <span class="math notranslate nohighlight">\(C_m^R\)</span> are the left and right child nodes and <span class="math notranslate nohighlight">\(f_L\)</span> and <span class="math notranslate nohighlight">\(f_R\)</span> are the fraction of observations going to each.</p>
<p>To allow for observation weighting, we simply change our definition of <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> to the following:</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_{mk} = \frac{\sum_{n \in \mathcal{N}_m} w_n I(y_n = k)}{\sum_{n \in \mathcal{N}_m} w_n }.
\]</div>
<p>This forces the tree to prioritize isolating the classes that were poorly classified by previous learners. The Gini index and cross-entropy are then defined as before and the rest of the tree is fit in the same way.</p>
</div>
<div class="section" id="the-discrete-adaboost-algorithm">
<h3>The Discrete AdaBoost Algorithm<a class="headerlink" href="#the-discrete-adaboost-algorithm" title="Permalink to this headline">¶</a></h3>
<p>The <em>Discrete AdaBoost</em> algorithm is outlined below. We start by setting the weights equal for each observation. Then we build <span class="math notranslate nohighlight">\(T\)</span> weighted weak learners (decision trees in our case). Each observation’s weight is updated according to two factors: the strength of the iteration’s learner and the accuracy of the learner on that observation. Specifically, the update for the weight on observation <span class="math notranslate nohighlight">\(n\)</span> at iteration <span class="math notranslate nohighlight">\(t\)</span> takes the form</p>
<div class="math notranslate nohighlight">
\[
w^{t+1}_n = w_n\exp(\alpha^t I^t_n),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha^t\)</span> is a measure of the accuracy of the learner and <span class="math notranslate nohighlight">\(I^t_n\)</span> indicates the learner misclassified observation <span class="math notranslate nohighlight">\(n\)</span>. This implies that the weight for a correctly-classified observation does not change while the weight for a misclassified observation increases, particularly if the model is accurate.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notation for boosting can get confusing since we iterate over learners and through observations. For the rest of this page, let superscripts refer to the model iteration and subscripts refer to the observation. For instance, <span class="math notranslate nohighlight">\(w^t_n\)</span> refers to the weight of observation <span class="math notranslate nohighlight">\(n\)</span> in learner <span class="math notranslate nohighlight">\(t\)</span>.</p>
</div>
<hr class="docutils" />
<p><strong>Discrete AdaBoost Algorithm</strong></p>
<p>Define the target variable to be <span class="math notranslate nohighlight">\(y_n \in \{-1, +1 \}\)</span>.</p>
<ol>
<li><p>Initialize the weights with <span class="math notranslate nohighlight">\(w^1_n = \frac{1}{N}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>. Hence if there are 10 datapoints, then each datapoint will have an inital weight of <span class="math notranslate nohighlight">\(\frac{1}{10}\) = 0.10</span>. </p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 1, \dots, T\)</span></p>
<ul>
<li><p>Build weak learner <span class="math notranslate nohighlight">\(t\)</span> using weights <span class="math notranslate nohighlight">\(\mathbf{w}^t\)</span>.</p></li>
<li><p>Use weak learner <span class="math notranslate nohighlight">\(t\)</span> to calculate fitted values <span class="math notranslate nohighlight">\(f^t(\bx_n) \in \{-1, +1\}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>. Let <span class="math notranslate nohighlight">\(I^t_n\)</span> equal 1 If <span class="math notranslate nohighlight">\(f^t(\bx_n) \neq y_n\)</span> and 0 otherwise. That is, <span class="math notranslate nohighlight">\(I^t_n\)</span> indicates whether learner <span class="math notranslate nohighlight">\(t\)</span> misclassifies observation <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p>Calculate the weighted error for learner <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \epsilon^t = \frac{\sumN w^t_n I^t_n}{\sumN w^t_n}.
      \]</div>
<p>Now assume that out of the 10 datapoints our weak learner manages to get 6 correct, using the above formulae:<span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \epsilon^1 = \frac{6*0*0.1 + 4*1*0.1}{10*0.1} = 0.40
      \]</div>
</li>
<li><p>Calculate the accuracy measure for learner <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \alpha^t = \frac{1}{2}\ln\left(\frac{1-\epsilon^t}{\epsilon^t}\right).
      \]</div>
<p>Continuing the calculation:<span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \alpha^t = \frac{1}{2}\ln\left(\frac{1-0.4}{0.4}\right) = 0.20275.
      \]</div>
<p>Notice that models with low error rate will have higher values of <span class="math notranslate nohighlight">\alpha^t</span>. Similarly, for a <span class="math notranslate nohighlight">\epsilon^t</span> value of 0.5 <span class="math notranslate nohighlight">\alpha^t</span> will be 0.</p>
</li>
<li><p>Update the weighting with</p>
<div class="math notranslate nohighlight">
\[
      w^{t + 1}_n = \frac{w^t_n\exp(-\alpha^t f^t(\bx_n)y_n\))}{Z^t},
      \]</div>
<p>for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>,</p>
<p>where <span class="math notranslate nohighlight">Z^t</span> is a normalization factor, chosen so that <span class="math notranslate nohighlight">w^t</span> is a distribution.</p>
<p>Now let's take a look at how the weights of 10 datapoints look post update:
<ul>
<li>For the datapoints which were correctly classified: <span class="math notranslate nohighlight"> w^{t + 1}_n = \frac{w^t_n\exp(-0.20275*1*1\))}{Z^t} = \frac{w^t_n * 0.82)}{Z_t}</span>
<p>Notice the weights are decreasing as the multiplicative factor becomes 0.82</p></li>
<li>For the datapoints which were incorrectly classified: <span class="math notranslate nohighlight"> w^{t + 1}_n = \frac{w^t_n\exp(-0.20275*-1*1\))}{Z^t} = \frac{w^t_n * 1.22)}{Z_t}</span>
<p>Notice the weights are increasing as the multiplicative factor becomes 1.22</p></li>
</ul>
  
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One must be careful while feeding data to an AdaBoost model, if there are outliers then the model will keep increasing the weight of those observations over and over again.</p>
</div>
<hr class="docutils" />
  
</ul>
</li>
<li><p>Calculate the overall fitted values with <span class="math notranslate nohighlight">\(\hat{y}_n = \text{sign} \left( \sum_{t = 1}^T \alpha^t f^t(\bx_n)\right)\)</span>.</p></li>
</ol>
<hr class="docutils" />
<p>Note that our final fitted value for observation <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>, is a weighted vote of all the individual fitted values where higher-performing learners are given more weight.</p>
</div>
</div>
<div class="section" id="adaboost-for-regression">
<h2>AdaBoost For Regression<a class="headerlink" href="#adaboost-for-regression" title="Permalink to this headline">¶</a></h2>
<p>We can apply the same principle of learning from our mistakes to regression tasks as well. A common algorithm for doing so is <em>AdaBoost.R2</em>, shown below. Like <em>AdaBoost</em>, this algorithm uses weights to emphasize observations that previous learners struggled with. Unlike <em>AdaBoost</em>, however, it does not incorporate these weights into the loss function directly. Instead, every iteration, it draws bootstrap samples from the training data where observations with greater weights are more likely to be drawn.</p>
<p>We then fit a weak learner to the bootstrapped sample, calculate the fitted values on the <em>original sample</em> (i.e. <em>not</em> the bootstrapped sample), and use the residuals to assess the quality of the weak learner. As in <em>AdaBoost</em>, we update the weights each iteration based on the quality of the learner (determined by <span class="math notranslate nohighlight">\(\beta^t\)</span>) and the accuracy of the learner on the observation (determined by <span class="math notranslate nohighlight">\(L_n\)</span>).</p>
<p>After iterating through many weak learners, we form our fitted values. Specifically, for each observation we use the weighted median (defined below) of the weak learners’ predictions, weighted by <span class="math notranslate nohighlight">\(\log(1/\beta^t)\)</span> for <span class="math notranslate nohighlight">\(t = 1, \dots, T\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Weighted Median</strong></p>
<p>Consider a set of values <span class="math notranslate nohighlight">\(\{v_1, v_2, \dots, v_N\}\)</span> and a set of corresponding weights <span class="math notranslate nohighlight">\(\{s_1, s_2, \dots, s_N\}\)</span>. Normalize the weights so they add to 1. To calculate the weighted median, sort the values and weights in ascending order of the values; call these <span class="math notranslate nohighlight">\(\{v^{(1)}, v^{(2)}, \dots, v^{(N)}\}\)</span> and  <span class="math notranslate nohighlight">\(\{s^{(1)}, s^{(2)}, \dots, s^{(N)}\}\)</span> where, for instance, <span class="math notranslate nohighlight">\(v^{(1)}\)</span> is the smallest value and <span class="math notranslate nohighlight">\(s^{(1)}\)</span> is its corresponding weight. The weighted median is then the value corresponding to the first weight such that the cumulative sum of the weights is greater than or equal to 0.5.</p>
<p>As an example, consider the values <span class="math notranslate nohighlight">\(\mathbf{v} = \{10,30,20,40\}\)</span> and corresponding weights <span class="math notranslate nohighlight">\(\mathbf{s} = \{0.4, 0.6, 0.2, 0.8\}\)</span>. First normalize the weights to sum to 1, giving <span class="math notranslate nohighlight">\(\mathbf{s} = \{0.2, 0.3, 0.1, 0.4\}\)</span>. Then sort the values and the weights, giving <span class="math notranslate nohighlight">\(\mathbf{v} = \{10, 20, 30, 40\}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{s} = \{0.2, 0.1, 0.3, 0.4\}\)</span>. Then, since the third element is the first for which the cumulative sum of the weights is at least 0.5, the weighted median is the third of the sorted values, <span class="math notranslate nohighlight">\(30\)</span>.</p>
</div>
<p>Though the arithmetic below gets somewhat messy, the concept is simple: iteratively fit a weak learner, see where the learner struggles, and emphasize the observations where it failed (where the amount of emphasis depends on the overall strength of the learner).</p>
<hr class="docutils" />
<p><strong>AdaBoost.R2 Algorithm</strong></p>
<ol>
<li><p>Initialize the weights with <span class="math notranslate nohighlight">\(w^1_n = \frac{1}{N}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 1, 2, \dots, T\)</span> or while <span class="math notranslate nohighlight">\(\bar{L}^t\)</span>, as defined below, is less than or equal to 0.5,</p>
<ul class="simple">
<li><p>Draw a sample of size <span class="math notranslate nohighlight">\(N\)</span> from the training data with replacement and with probability <span class="math notranslate nohighlight">\(w^t_n\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>Fit weak learner <span class="math notranslate nohighlight">\(t\)</span> to the resampled data and calculate the fitted values on the original dataset. Denote these fitted values with <span class="math notranslate nohighlight">\(f^t(\bx_{n})\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>Calculate the observation error <span class="math notranslate nohighlight">\(L^t_{n}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    D^t &amp;= \underset{n}{\text{max}} \{ |y_{n} - f^t(\bx_{n})|  \} \\
    L^t_{n} &amp;= \frac{|y_{n} - f^t(\bx_{n})|}{D^t}
    \end{aligned}
    \end{split}\]</div>
<ul>
<li><p>Calculate the model error <span class="math notranslate nohighlight">\(\bar{L}^t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \bar{L}^t = \sum_{n = 1}^N  L^t_n w^t_n
      \]</div>
<p>If <span class="math notranslate nohighlight">\(\bar{L}^t \geq 0.5\)</span>, end iteration and set <span class="math notranslate nohighlight">\(T\)</span> equal to <span class="math notranslate nohighlight">\(t - 1\)</span>.</p>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(\beta^t = \frac{\bar{L}^t}{1- \bar{L}^t}\)</span>. The lower <span class="math notranslate nohighlight">\(\beta^t\)</span>, the greater our confidence in the model.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(Z^t = \sum_{n = 1}^N w^t_n (\beta^t)^{1 - L^t_n}\)</span> be a normalizing constant and update the model weights with</p>
<div class="math notranslate nohighlight">
\[
      w^{t + 1}_n = \frac{w^t_n (\beta^t)^{1 - L^t_n}}{Z^t},
      \]</div>
<p>which increases the weight for observations with a greater error <span class="math notranslate nohighlight">\(L^t_n\)</span>.</p>
</li>
</ul>
</li>
<li><p>Set the overall fitted value for observation <span class="math notranslate nohighlight">\(n\)</span> equal to the weighted median of <span class="math notranslate nohighlight">\(f^t(\bx_n)\)</span> for <span class="math notranslate nohighlight">\(t = 1, 2, \dots, T\)</span> using weights <span class="math notranslate nohighlight">\(\log(1/\beta^t)\)</span> for model <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ol>
<hr class="docutils" />
<p>In the boosting <a class="reference internal" href="../s2/boosting.html"><span class="doc">construction</span></a>, we implement <em>AdaBoost.R2</em> using decision tree regressors though many other weak learners may be used.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/c6/s1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="random_forests.html" title="previous page">Random Forests</a>
    <a class='right-next' id="next-link" href="../construction.html" title="next page">Construction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
  </body>
</html>
