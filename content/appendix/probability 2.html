

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Probability &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Common Methods" href="methods.html" />
    <link rel="prev" title="Math" href="math.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../table_of_contents.html">
   Table of Contents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conventions_notation.html">
   Conventions and Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  1. Ordinary Linear Regression
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c1/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c1/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  2. Linear Regression Extensions
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c2/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c2/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c2/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  3. Discriminative Classifiers (Logistic Regression)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c3/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c3/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c3/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  4. Generative Classifiers (Naive Bayes)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c4/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c4/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c4/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  5. Decision Trees
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c5/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c5/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c5/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  6. Tree Ensemble Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c6/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c6/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c6/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  7. Neural Networks
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../c7/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c7/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../c7/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="math.html">
   Math
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="methods.html">
   Common Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   Datasets
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/appendix/probability.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dafriedman97/mlbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dafriedman97/mlbook/issues/new?title=Issue%20on%20page%20%2Fcontent/appendix/probability.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/dafriedman97/mlbook/edit/master/content/appendix/probability.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-variables-and-distributions">
   1. Random Variables and Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-variables">
     Random Variables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#density-functions">
     Density Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributions">
     Distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence">
     Independence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
   2. Maximum Likelihood Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-probability">
   3. Conditional Probability
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="probability">
<h1>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h1>
<p>Many machine learning methods are rooted in probability theory. Probabilistic methods in this book include <a class="reference internal" href="../c1/concept.html"><span class="doc">linear regression</span></a>, <a class="reference internal" href="../c2/s1/bayesian.html"><span class="doc">Bayesian regression</span></a>, and <a class="reference internal" href="../c4/concept.html"><span class="doc">generative classifiers</span></a>. This section covers the probability theory needed to understand those methods.</p>
<div class="section" id="random-variables-and-distributions">
<h2>1. Random Variables and Distributions<a class="headerlink" href="#random-variables-and-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="random-variables">
<h3>Random Variables<a class="headerlink" href="#random-variables" title="Permalink to this headline">¶</a></h3>
<p>A <strong>random variable</strong> is a variable whose value is randomly determined. The set of possible values a random variable can take on is called the variable’s <strong>support</strong>. An example of a random variable is the value on a die roll. This variable’s support is <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span>. Random variables will be represented with uppercase letters and values in their support with lowercase letters. For instance <span class="math notranslate nohighlight">\(X = x\)</span> implies that a random variable <span class="math notranslate nohighlight">\(X\)</span> happened to take on value <span class="math notranslate nohighlight">\(x\)</span>. Letting <span class="math notranslate nohighlight">\(X\)</span> be the value of a die roll, <span class="math notranslate nohighlight">\(X = 4\)</span> indicates that the die landed on 4.</p>
</div>
<div class="section" id="density-functions">
<h3>Density Functions<a class="headerlink" href="#density-functions" title="Permalink to this headline">¶</a></h3>
<p>The likelihood that a random variable takes on a given value is determined through its density function. For a discrete random variable (one that can take on a finite set of values), this density function is called the <strong>probability mass function</strong> <strong>(PMF)</strong>. The PMF of a random variable <span class="math notranslate nohighlight">\(X\)</span> gives the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal some value <span class="math notranslate nohighlight">\(x\)</span>. We write it as <span class="math notranslate nohighlight">\(f_X(x)\)</span> or just <span class="math notranslate nohighlight">\(f(x)\)</span>, and it is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = P(X = x).
\]</div>
<p>For a continuous random variable (one that can take on infinitely many values), the density function is called the <strong>probability density function (PDF)</strong>. The PDF <span class="math notranslate nohighlight">\(f_X(x)\)</span> of a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> does not give <span class="math notranslate nohighlight">\(P(X = x)\)</span> but it does determine the probability that <span class="math notranslate nohighlight">\(X\)</span> lands in a certain range. Specifically,</p>
<div class="math notranslate nohighlight">
\[
P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. 
\]</div>
<p>That is, integrating <span class="math notranslate nohighlight">\(f(x)\)</span> over a certain range gives the probability of <span class="math notranslate nohighlight">\(X\)</span> being in that range. While <span class="math notranslate nohighlight">\(f(x)\)</span> does not give the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal a certain value, it does indicate the relative likelihood that it will be <em>around</em> that value. E.g. if <span class="math notranslate nohighlight">\(f(a) &gt; f(b)\)</span>, we can say <span class="math notranslate nohighlight">\(X\)</span> is more likely to be in an arbitrarily small area around the value <span class="math notranslate nohighlight">\(a\)</span> than around the value <span class="math notranslate nohighlight">\(b\)</span>.</p>
</div>
<div class="section" id="distributions">
<h3>Distributions<a class="headerlink" href="#distributions" title="Permalink to this headline">¶</a></h3>
<p>A random variable’s <strong>distribution</strong> is determined by its density function. Variables with the same density function are said to follow the same distributions. Certain families of distributions are very common in probability and machine learning. Two examples are given below.</p>
<p>The <strong>Bernoulli</strong> distribution is the most simple probability distribution and it describes the likelihood of the outcomes of a binary event. Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable that equals 1 (representing “success”) with probability <span class="math notranslate nohighlight">\(p\)</span> and 0 (representing “failure”) with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Then, <span class="math notranslate nohighlight">\(X\)</span> is said to follow the Bernoulli distribution with probability parameter <span class="math notranslate nohighlight">\(p\)</span>, written <span class="math notranslate nohighlight">\(X \sim \text{Bern}(p)\)</span>, and its PMF is given by</p>
<div class="math notranslate nohighlight">
\[
f_X(x) = p^x(1-p)^{(1-x)}.
\]</div>
<p>We can check to see that for any valid value <span class="math notranslate nohighlight">\(x\)</span> in the support of <span class="math notranslate nohighlight">\(X\)</span>—i.e., 1 or 0—, <span class="math notranslate nohighlight">\(f(x)\)</span> gives <span class="math notranslate nohighlight">\(P(X = x)\)</span>.</p>
<p>The <strong>Normal</strong> distribution is extremely common and will be used throughout this book. A random variable <span class="math notranslate nohighlight">\(X\)</span> follows the Normal distribution with mean parameter <span class="math notranslate nohighlight">\(\mu \in \R\)</span> and variance parameter <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>, written <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, if its PDF is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
\]</div>
<p>The shape of the Normal random variable’s density function gives this distribution the name “the bell curve”, as shown below. Values closest to <span class="math notranslate nohighlight">\(\mu\)</span> are most likely and the density is symmetric around <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><img alt="normal" src="../../_images/normal.jpg" /></p>
</div>
<div class="section" id="independence">
<h3>Independence<a class="headerlink" href="#independence" title="Permalink to this headline">¶</a></h3>
<p>So far we’ve discussed the density of individual random variables. The picture can get much more complicated when we want to study the behavior of multiple random variables simultaneously. The assumption of independence simplifies things greatly. Let’s start by defining independence in the discrete case.</p>
<p>Two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if and only if</p>
<div class="math notranslate nohighlight">
\[
P(X = x, Y =y) = P(X = x)P(Y = y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This says that if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, the probability that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> simultaneously is just the product of the probabilities that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> individually.</p>
<p>To generalize this definition to continuous random variables, let’s first introduce <em>joint density function</em>. Quite simply, the joint density of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, written <span class="math notranslate nohighlight">\(f_{X, Y}(x, y)\)</span> gives the probability density of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> evaluated simultaneously at <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, respectively. We can then say that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if and only if</p>
<div class="math notranslate nohighlight">
\[
f_{X, Y}(x, y) = f_X(x) f_Y(y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
</div>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h2>2. Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h2>
<p>Maximum likelihood estimation is used to understand the parameters of a distribution that gave rise to observed data. In order to model a data generating process, we often assume it comes from some family of distributions, such as the Bernoulli or Normal distributions. These distributions are indexed by certain parameters (<span class="math notranslate nohighlight">\(p\)</span> for the Bernoulli and <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> for the Normal)—maximum likelihood estimation evaluates which parameters would be most consistent with the data we observed.</p>
<p>Specifically, maximum likelihood estimation finds the values of unknown parameters that maximize the probability of observing the data we did. Basic maximum likelihood estimation can be broken into three steps:</p>
<ol class="simple">
<li><p>Find the joint density of the observed data, also called the <em>likelihood</em></p></li>
<li><p>Take the log of the likelihood, giving the <em>log-likelihood</em>.</p></li>
<li><p>Find the value of the parameter that maximizes the log-likelihood (and therefore the likelihood as well) by setting its derivative equal to 0.</p></li>
</ol>
<p>Finding the value of the parameter to maximize the log-likelihood rather than the likelihood makes the math easier and gives us the same solution.</p>
<p>Let’s go through an example. Suppose we are interested in calculating the average weight of a Chihuahua. We assume the weight of any given Chihuahua is <em>independently</em> distributed Normally with <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span> but an unknown mean <span class="math notranslate nohighlight">\(\mu\)</span>. So, we gather 10 Chihuahuas and weigh them. Denote the <span class="math notranslate nohighlight">\(j^\text{th}\)</span> Chihuahua weight with <span class="math notranslate nohighlight">\(W_j \sim \mathcal{N}(\mu, 1)\)</span>.  For step 1, let’s calculate the probability density of our data (i.e., the 10 Chihuahua weights). Since the weights are assumed to be independent, the densities multiply. Letting <span class="math notranslate nohighlight">\(L(\mu)\)</span> be the likelihood of <span class="math notranslate nohighlight">\(\mu\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
L(\mu) &amp;= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10}) \\
&amp;= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10})  \\
&amp;= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot 1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\
&amp;\propto \exp\left(-\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\
\end{align}
\end{split}\]</div>
<p>Note that we can work up to a constant of proportionality since the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes <span class="math notranslate nohighlight">\(L(\mu)\)</span> will also maximize anything proportional to <span class="math notranslate nohighlight">\(L(\mu)\)</span>. For step 2, take the log:</p>
<div class="math notranslate nohighlight">
\[
\log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> is some constant. For step 3, take the derivative:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j - \mu).
\end{align}
\]</div>
<p>Setting this equal to 0, we find that the (log) likelihood is maximized with</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. 
\]</div>
<p>We put a hat over <span class="math notranslate nohighlight">\(\mu\)</span> to indicate that it is our <em>estimate</em> of the true <span class="math notranslate nohighlight">\(\mu\)</span>. Note the sensible result—we estimate the true mean of the Chihuahua weight distribution to be the sample mean of our observed data.</p>
</div>
<div class="section" id="conditional-probability">
<h2>3. Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic machine learning methods typically consider the distribution of a target variable conditional on the value of one or more predictor variables. To understand these methods, let’s introduce some of the basic principles of conditional probability.</p>
<p>Consider two events, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. The <strong>conditional probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is the probability that <span class="math notranslate nohighlight">\(A\)</span> occurs given <span class="math notranslate nohighlight">\(B\)</span> occurs, written <span class="math notranslate nohighlight">\(P(A|B)\)</span>. Closely related is the <strong>joint probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, or the probability that both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occur, written <span class="math notranslate nohighlight">\(P(A, B)\)</span>. We navigate between the conditional and joint probability with the following</p>
<div class="math notranslate nohighlight">
\[
P(A, B) = P(A|B)P(B).
\]</div>
<p>The above equation leads to an extremely important principle in conditional probability: Bayes’ rule. <strong>Bayes’ rule</strong> states that</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
\]</div>
<p>Both of the above expressions work for random variables as well as events. For any two discrete random variables, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
P(X = x, Y = y) &amp;= P(X = x|Y = y)P(Y = y) \\
P(X = x|Y = y) &amp;= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}.
\end{align}
\end{split}\]</div>
<p>The same is true for continuous random variables, replacing the PMFs with PDFs.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/appendix"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="math.html" title="previous page">Math</a>
    <a class='right-next' id="next-link" href="methods.html" title="next page">Common Methods</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../_static/js/index.js"></script>
    
  </body>
</html>