

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Regression Trees &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/mystnb.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}})</script>
    <link rel="shortcut icon" href="../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Classification Trees" href="classification_tree.html" />
    <link rel="prev" title="Concept" href="../concept.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../table_of_contents.html">Table of Contents</a>
  </li>
  <li class="">
    <a href="../../conventions_notation.html">Conventions and Notation</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">1. Ordinary Linear Regression</p>
</li>
  <li class="">
    <a href="../../c1/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c1/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c1/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">2. Linear Regression Extensions</p>
</li>
  <li class="">
    <a href="../../c2/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c2/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c2/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">3. Discriminative Classifiers (Logistic Regression)</p>
</li>
  <li class="">
    <a href="../../c3/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c3/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c3/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">4. Generative Classifiers (Naive Bayes)</p>
</li>
  <li class="">
    <a href="../../c4/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c4/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c4/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">5. Decision Trees</p>
</li>
  <li class="active">
    <a href="../concept.html">Concept</a>
  <ul class="nav sidenav_l2">
    <li class="active">
      <a href="">Regression Trees</a>
    </li>
    <li class="">
      <a href="classification_tree.html">Classification Trees</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">6. Tree Ensemble Methods</p>
</li>
  <li class="">
    <a href="../../c6/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c6/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c6/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">7. Neural Networks</p>
</li>
  <li class="">
    <a href="../../c7/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c7/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c7/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Appendix</p>
</li>
  <li class="">
    <a href="../../appendix/math.html">Math</a>
  </li>
  <li class="">
    <a href="../../appendix/probability.html">Probability</a>
  </li>
  <li class="">
    <a href="../../appendix/methods.html">Common Methods</a>
  </li>
  <li class="">
    <a href="../../appendix/data.html">Datasets</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../../../_sources/content/c5/s1/regression_tree.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        <a class="edit-button" href="https://github.com/dafriedman97/book/edit/master/content/c5/s1/regression_tree.md"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" title="Edit this page"><i class="fas fa-pencil-alt"></i></button></a>

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#building-a-tree" class="nav-link">Building a Tree</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#defining-rules" class="nav-link">Defining Rules</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#the-objective" class="nav-link">The Objective</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#making-splits" class="nav-link">Making Splits</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#making-predictions" class="nav-link">Making Predictions</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#choosing-hyperparameters" class="nav-link">Choosing Hyperparameters</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#size-regulation" class="nav-link">Size Regulation</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#minimum-reduction-in-rss" class="nav-link">Minimum Reduction in RSS</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#pruning" class="nav-link">Pruning</a>
        </li>
    
            </ul>
        </li>
    
    </ul>
</nav>



<div class="tocsection editthispage">
    <a href="https://github.com/dafriedman97/book/edit/master/content/c5/s1/regression_tree.md">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="regression-trees">
<h1>Regression Trees<a class="headerlink" href="#regression-trees" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\newcommand{\R}{\mathbb{R}}
\]</div>
<p>Decision trees are deeply rooted in tree-based terminology. Before discussing decision trees in depth, let’s go over some of this vocabulary.</p>
<ul class="simple">
<li><p><strong>Node</strong>: A node is comprised of a sample of data and a decision rule.</p></li>
<li><p><strong>Parent</strong>, <strong>Child</strong>: A parent is a node in a tree associated with exactly two child nodes. Observations directed to a parent node are next directed to one of that parent’s two children nodes.</p></li>
<li><p><strong>Rule</strong>: Each parent node has a rule that determines toward which of the child nodes an observation will be directed. The rule is based on the value of one (and only one) of an observation’s predictors.</p></li>
<li><p><strong>Buds</strong>: Buds are nodes that have not yet been split. In other words, they are children that are eligible to become parents. Nodes are only considered buds during the training process. Nodes that are never split are considered leaves.</p></li>
<li><p><strong>Leaf</strong>: Leaves, also known as <em>terminal nodes</em>, are nodes that are never split. Observations move through a decision tree until reaching a leaf. The fitted value of a test observation is determined by the training observations that land in the same leaf.</p></li>
</ul>
<p>Let’s return to the tree introduced in the previous page to see these vocabulary terms in action. Each square in this tree is a node. The sample of data for node <span class="math notranslate nohighlight">\(A\)</span> is the collection of penguins with flipper lengths under 206.5mm and the decision rule for node <span class="math notranslate nohighlight">\(A\)</span> is whether the bill length is less than or equal to 43.35mm. Node <span class="math notranslate nohighlight">\(A\)</span> is an example of a parent node, and nodes <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are examples of child nodes. Before node <span class="math notranslate nohighlight">\(A\)</span> was split into child nodes <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span>, it was an example of a bud. Since <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are never split, they are examples of leaves.</p>
<p><img alt="tree" src="../../../_images/tree1.png" /></p>
<p>The rest of this section describes how to fit a decision tree. Our tree starts with an initial node containing all the training data. We then assign this node a rule, which leads to the creation of two child nodes. Training observations are assigned to one of these two child nodes based on their response to the rule. How these rules are made is discussed in the first <a class="reference internal" href="#building-a-tree"><span class="std std-ref">sub-section</span></a> below.</p>
<p>Next, these child nodes are considered <em>buds</em> and are ready to become parent nodes. Each bud is assigned a rule and split into two child nodes. We then have four <em>buds</em> and each is once again split. We continue this process until some stopping rule is reached (discussed <a class="reference internal" href="#choosing-hyperparameters"><span class="std std-ref">later</span></a>). Finally, we run test observations through the tree and assign them fitted values according to the leaf they fall in.</p>
<p>In this section we’ll discuss how to build a tree as well as how to choose the tree’s optimal hyperparameters.</p>
<div class="section" id="building-a-tree">
<span id="id1"></span><h2>Building a Tree<a class="headerlink" href="#building-a-tree" title="Permalink to this headline">¶</a></h2>
<p>Building a tree consists of iteratively creating rules to split nodes. We’ll first discuss rules in greater depth, then introduce a tree’s objective function, then cover the splitting process, and finally go over making predictions with a built tree. Suppose we have predictors <span class="math notranslate nohighlight">\(\bx_n \in \R^D\)</span> and a quantitative target variable <span class="math notranslate nohighlight">\(y_n\)</span> for <span class="math notranslate nohighlight">\(n = 1, \dots, N\)</span>.</p>
<div class="section" id="defining-rules">
<h3>Defining Rules<a class="headerlink" href="#defining-rules" title="Permalink to this headline">¶</a></h3>
<p>Let’s first clarify what a rule actually is. Rules in a decision tree determine how observations from a parent node are divided between two child nodes. Each rule is based on only <em>one</em> predictor from the parent node’s training sample. Let <span class="math notranslate nohighlight">\(x_{nd}\)</span> be the <span class="math notranslate nohighlight">\(d^\text{th}\)</span> predictor in <span class="math notranslate nohighlight">\(\bx_n\)</span>. If <span class="math notranslate nohighlight">\(x_{nd}\)</span> is quantitative, a rule is of the form</p>
<div class="math notranslate nohighlight">
\[
x_{nd} \leq t,
\]</div>
<p>for some <em>threshold</em> value <span class="math notranslate nohighlight">\(t\)</span>. If <span class="math notranslate nohighlight">\(x_{nd}\)</span> is categorical, a rule is of the form</p>
<div class="math notranslate nohighlight">
\[
x_{nd} \in \mathcal{S},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is a set of possible values of the <span class="math notranslate nohighlight">\(d^\text{th}\)</span> predictor.</p>
<p>Let’s introduce a little notation to clear things up. Let <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> represent the tree’s <span class="math notranslate nohighlight">\(m^\text{th}\)</span> node and <span class="math notranslate nohighlight">\(\mathcal{R}_m\)</span> be the rule for <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span>. Then let <span class="math notranslate nohighlight">\(\mathcal{C}^L_m\)</span> and <span class="math notranslate nohighlight">\(\mathcal{C}_m^R\)</span> be the child nodes of <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span>. As a convention, suppose observations are directed to <span class="math notranslate nohighlight">\(\mathcal{C}_m^L\)</span> if they satisfy the rule and <span class="math notranslate nohighlight">\(\mathcal{C}_m^R\)</span> otherwise. For instance, let</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_m = \left( x_{n2} \leq 4 \right).
\]</div>
<p>Then observation <span class="math notranslate nohighlight">\(n\)</span> that passes through <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> will go to <span class="math notranslate nohighlight">\(\mathcal{C}_m^L\)</span> if <span class="math notranslate nohighlight">\(x_{n2} \leq 4\)</span> and <span class="math notranslate nohighlight">\(\mathcal{C}_m^R\)</span> otherwise. On a diagram like the one above, the child node for observations satisfying the rule is often to the left while the node for those not satisfying the rule is to the right, hence the superscripts <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(R\)</span></p>
</div>
<div class="section" id="the-objective">
<h3>The Objective<a class="headerlink" href="#the-objective" title="Permalink to this headline">¶</a></h3>
<p>Consider first a single node <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span>. Let <span class="math notranslate nohighlight">\(n \in \mathcal{N}_m\)</span>  be the collection of training observations that pass through the node and <span class="math notranslate nohighlight">\(\bar{y}_m\)</span> be the sample mean of these observations. The residual sum of squares (<span class="math notranslate nohighlight">\(RSS\)</span>) for <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
RSS_m = \sum_{n \in \mathcal{N}_m} \left(y_n - \bar{y}_m \right)^2.
\]</div>
<p>The loss function for the entire tree is the <span class="math notranslate nohighlight">\(RSS\)</span> across buds (if still being fit) or across leaves (if finished fitting). Letting <span class="math notranslate nohighlight">\(I_m\)</span> be an indicator that node <span class="math notranslate nohighlight">\(m\)</span> is a leaf or bud (i.e. <em>not</em> a parent), the total loss for the tree is written as</p>
<div class="math notranslate nohighlight">
\[
RSS_T = \sum_m \sum_{n \in \mathcal{N}_m} I_m RSS_m.
\]</div>
<p>In choosing splits, we hope to reduce <span class="math notranslate nohighlight">\(RSS_T\)</span> as much as possible. We can write the reduction in <span class="math notranslate nohighlight">\(RSS_T\)</span> from splitting bud <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> into children <span class="math notranslate nohighlight">\(\mathcal{C}_m^L\)</span> and <span class="math notranslate nohighlight">\(\mathcal{C}_m^R\)</span> as the <span class="math notranslate nohighlight">\(RSS\)</span> of the bud minus the sum of the <span class="math notranslate nohighlight">\(RSS\)</span> of its children, or</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\Delta RSS_T &amp;= RSS _m - \left(RSS_{\mathcal{C}_m^L} + RSS_{\mathcal{C}_m^R} \right) 
\\ &amp;= \sum_{n \in \mathcal{N}_m} (y_n - \bar{y}_m)^2 - \left(\sum_{n \in \mathcal{C}_m^L} \left(y_n - \bar{y}_m^L\right)^2 + \sum_{n \in \mathcal{C}_m^R} \left(y_n - \bar{y}_m^R\right)^2\right)
\end{align*},
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{y}_m^L\)</span> and <span class="math notranslate nohighlight">\(\bar{y}_m^R\)</span> are the sample mean of the child nodes.</p>
</div>
<div class="section" id="making-splits">
<h3>Making Splits<a class="headerlink" href="#making-splits" title="Permalink to this headline">¶</a></h3>
<p>Finding the optimal sequence of splits to minimize <span class="math notranslate nohighlight">\(RSS_T\)</span> becomes a combinatorially infeasible task as we add predictors and observations. Instead, we take a greedy approach known as <em>recursive binary splitting</em>. When splitting each bud, we consider all possible predictors and all possible ways to split that predictor. If the predictor is quantitative, this means considering all possible thresholds for splitting. If the predictor is categorical, this means considering all ways to split the categories into two groups. After considering all possible splits, we choose the one with the greatest reduction in the bud’s <span class="math notranslate nohighlight">\(RSS\)</span>. This process is detailed below.</p>
<p>Note that once a node is split, what happens to one of its children is independent of what happens to the other. We can therefore build the tree in layers—first splitting the initial node, then splitting each of the initial node’s children, then splitting all four of those children’s children, etc.</p>
<p>For each layer (starting with only the initial node), we split with the following process.</p>
<ul>
<li><p>For each bud <span class="math notranslate nohighlight">\(m\)</span> on that layer,</p>
<ul>
<li><p>For each predictor <span class="math notranslate nohighlight">\(d\)</span>,</p>
<ul>
<li><p>If the predictor is quantitative:</p>
<ul>
<li><p>For each value of that predictor among the observations in the bud <a class="footnote-reference brackets" href="#footnote" id="id2">1</a>,</p>
<ul>
<li><p>Let this value be the threshold value <span class="math notranslate nohighlight">\(t\)</span> and consider the reduction in <span class="math notranslate nohighlight">\(RSS_m\)</span> from splitting at this threshold. I.e. consider the reduction in <span class="math notranslate nohighlight">\(RSS_m\)</span> from the rule</p>
<div class="math notranslate nohighlight">
\[
          \mathcal{R} = x_{nd} \leq t.
          \]</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p>If the predictor is categorical:</p>
<ul>
<li><p>Rank the categories according to the average value of the target variable for observations in each category. If there are <span class="math notranslate nohighlight">\(V\)</span> distinct values, index these categories <span class="math notranslate nohighlight">\(c_1, \dots, c_V\)</span> in ascending order.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(v = 1, \dots, V-1\)</span> <a class="footnote-reference brackets" href="#footnote" id="id3">1</a>,</p>
<ul>
<li><p>Consider the set <span class="math notranslate nohighlight">\(\mathcal{S} = \{ c_1, \dots, c_v\}\)</span> and consider the reduction in <span class="math notranslate nohighlight">\(RSS_m\)</span> from the rule</p>
<div class="math notranslate nohighlight">
\[
          \mathcal{R} = x_{nd} \in \mathcal{S}.
          \]</div>
</li>
</ul>
<p>In words, we consider the set of categories corresponding to the lowest average target variable and split if the observation’s <span class="math notranslate nohighlight">\(d^\text{th}\)</span> predictor falls in this set. We then add one more category to this set and repeat.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Choose the predictor, and value (if quantitative) or set (if categorical) with the greatest reduction in <span class="math notranslate nohighlight">\(RSS_m\)</span> and split accordingly.</p></li>
</ul>
</li>
</ul>
<p>We repeat this process until some <a class="reference internal" href="#choosing-hyperparameters"><span class="std std-ref">stopping rule</span></a> is reached.</p>
</div>
<div class="section" id="making-predictions">
<h3>Making Predictions<a class="headerlink" href="#making-predictions" title="Permalink to this headline">¶</a></h3>
<p>Making predictions with a built decision tree is very straightforward. For each test observation, we simply run the observation through the built tree. Once it reaches a leaf (a terminal node), we predict its target variable to be the sample mean of the training observations in that leaf.</p>
</div>
</div>
<div class="section" id="choosing-hyperparameters">
<span id="id4"></span><h2>Choosing Hyperparameters<a class="headerlink" href="#choosing-hyperparameters" title="Permalink to this headline">¶</a></h2>
<p>As with any bias-variance tradeoff issue, we want our tree to learn valuable patterns from the data without reading into the idiosyncrasies of our particular training set. Without any sort of regulation, a tree will minimize this total <span class="math notranslate nohighlight">\(RSS\)</span> by moving each training observation into its own leaf, causing obvious overfitting. Below are three common methods for limiting the depth of a tree.</p>
<div class="section" id="size-regulation">
<h3>Size Regulation<a class="headerlink" href="#size-regulation" title="Permalink to this headline">¶</a></h3>
<p>A simple way to limit a tree’s size is to directly regulate its depth, the size of its terminal nodes, or both.</p>
<p>We can define the <em>depth</em> of a node as the number of parent nodes that have come before it. For instance, the initial node has depth 0, the children of the first split have depth 1, and the children of the second split have depth 2. We can prevent a tree from overfitting by setting a maximum depth. To do this, we simply restrict nodes of this depth from being split.</p>
<p>The <em>size</em> of a node is defined as the number of training observations in that node. We can also prevent a tree from overfitting by setting a minimum size for parent nodes or child nodes. When setting a minimum size for parent nodes, we restrict nodes from splitting if they are below a certain size. When setting a minimum size for child nodes, we ban any splits that would create child nodes smaller than a certain size.</p>
<p>These parameters are typically chosen with <a class="reference internal" href="../../appendix/methods.html"><span class="doc">cross validation</span></a>.</p>
</div>
<div class="section" id="minimum-reduction-in-rss">
<h3>Minimum Reduction in RSS<a class="headerlink" href="#minimum-reduction-in-rss" title="Permalink to this headline">¶</a></h3>
<p>Another simple method for avoiding overfitting is to require that any split decrease the residual sum of squares, <span class="math notranslate nohighlight">\(RSS_T\)</span>, by a certain amount. Ideally, this allows the tree to separate groups that are sufficiently dissimilar while preventing it from making needless splits. Again, the exact required reduction in <span class="math notranslate nohighlight">\(RSS_T\)</span> should be chosen through cross validation.</p>
<p>One potential pitfall of this method is that early splits might elicit small decreases in <span class="math notranslate nohighlight">\(RSS_T\)</span> while later splits will elicit much larger ones. By requiring a minimum reduction in <span class="math notranslate nohighlight">\(RSS_T\)</span>, however, we might not reach these later splits. For instance, consider the dataset below with input variables <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> and target variable <span class="math notranslate nohighlight">\(y\)</span>. The first split, whether by <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(b\)</span>, will not reduce <span class="math notranslate nohighlight">\(RSS_T\)</span> at all while the second split would reduce it to 0.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>a</p></th>
<th class="head"><p>b</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="pruning">
<h3>Pruning<a class="headerlink" href="#pruning" title="Permalink to this headline">¶</a></h3>
<p>A third strategy to manage a decision tree’s size is through <em>pruning</em>. To prune a tree, first intentionally overfit it by, for instance, setting a low minimum node size or a high maximum depth. Then, just like pruning a literal tree, we cut back the unnecessary branches. Specifically, one by one we undue the least helpful split by re-joining terminal leaves. At any given step, we undue the split that causes the least increase in <span class="math notranslate nohighlight">\(RSS_T\)</span>.</p>
<p>When pruning a tree, we want to balance tree size with residual error. Define the size of a tree (not to be confused with the size of a <em>node</em>) to equal its number of terminal leaves, denoted <span class="math notranslate nohighlight">\(|T|\)</span> for tree <span class="math notranslate nohighlight">\(T\)</span>. The residual error for tree <span class="math notranslate nohighlight">\(T\)</span> is defined by <span class="math notranslate nohighlight">\(RSS_T\)</span>. To compare possible trees when pruning, we use the following loss function</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(T) = RSS_T +\lambda|T| ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is a regularization parameter. We choose the tree <span class="math notranslate nohighlight">\(T\)</span> that minimizes <span class="math notranslate nohighlight">\(\mathcal{L}(T)\)</span>.</p>
<p>Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> increases the amount of regularization and limits overfitting by penalizing larger trees. Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> too much, however, can cause undercutting by preventing the tree from making needed splits. How then do we choose <span class="math notranslate nohighlight">\(\lambda\)</span>? With cross validation! Specifically,  for each validation fold, we overfit the tree and prune it back using a range of <span class="math notranslate nohighlight">\(\lambda\)</span> values. Note that <span class="math notranslate nohighlight">\(\lambda\)</span> does not affect <em>how</em> we prune the tree, only <em>which</em> pruned tree we prefer. For each value of <span class="math notranslate nohighlight">\(\lambda\)</span>, we calculate the average validation accuracy (likely using <span class="math notranslate nohighlight">\(RSS_T\)</span> calculated on the validation set) across folds. We choose the value of <span class="math notranslate nohighlight">\(\lambda\)</span> that maximizes this accuracy and the tree which is preferred by that value of <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="footnote"><span class="brackets">1</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>For a quantitative predictor, we actually consider all but the largest. If we chose the rule <span class="math notranslate nohighlight">\(R = x_{nd} \leq t\)</span> and <span class="math notranslate nohighlight">\(t\)</span> was the largest value, this would not leave any observations in the second child. Similarly, for a categorical predictor, we do not consider the <span class="math notranslate nohighlight">\(V^\text{th}\)</span> category since this would not leave any observations for the second child.</p>
</dd>
</dl>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../concept.html" title="previous page">Concept</a>
    <a class='right-next' id="next-link" href="classification_tree.html" title="next page">Classification Trees</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
  </body>
</html>