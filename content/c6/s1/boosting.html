

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Boosting &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/mystnb.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}})</script>
    <link rel="shortcut icon" href="../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Construction" href="../construction.html" />
    <link rel="prev" title="Random Forests" href="random_forests.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../table_of_contents.html">Table of Contents</a>
  </li>
  <li class="">
    <a href="../../conventions_notation.html">Conventions and Notation</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">1. Ordinary Linear Regression</p>
</li>
  <li class="">
    <a href="../../c1/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c1/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c1/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">2. Linear Regression Extensions</p>
</li>
  <li class="">
    <a href="../../c2/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c2/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c2/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">3. Discriminative Classifiers (Logistic Regression)</p>
</li>
  <li class="">
    <a href="../../c3/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c3/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c3/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">4. Generative Classifiers (Naive Bayes)</p>
</li>
  <li class="">
    <a href="../../c4/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c4/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c4/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">5. Decision Trees</p>
</li>
  <li class="">
    <a href="../../c5/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c5/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c5/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">6. Tree Ensemble Methods</p>
</li>
  <li class="active">
    <a href="../concept.html">Concept</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="bagging.html">Bagging</a>
    </li>
    <li class="">
      <a href="random_forests.html">Random Forests</a>
    </li>
    <li class="active">
      <a href="">Boosting</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">7. Neural Networks</p>
</li>
  <li class="">
    <a href="../../c7/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c7/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c7/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Appendix</p>
</li>
  <li class="">
    <a href="../../appendix/math.html">Math</a>
  </li>
  <li class="">
    <a href="../../appendix/probability.html">Probability</a>
  </li>
  <li class="">
    <a href="../../appendix/methods.html">Common Methods</a>
  </li>
  <li class="">
    <a href="../../appendix/data.html">Datasets</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../../../_sources/content/c6/s1/boosting.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        <a class="edit-button" href="https://github.com/dafriedman97/mlbook/edit/master/content/c6/s1/boosting.md"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" title="Edit this page"><i class="fas fa-pencil-alt"></i></button></a>

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#adaboost-for-binary-classification" class="nav-link">AdaBoost for Binary Classification</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#weighted-classification-trees" class="nav-link">Weighted Classification Trees</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#the-discrete-adaboost-algorithm" class="nav-link">The Discrete AdaBoost Algorithm</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#adaboost-for-regression" class="nav-link">AdaBoost For Regression</a>
        </li>
    
    </ul>
</nav>



<div class="tocsection editthispage">
    <a href="https://github.com/dafriedman97/mlbook/edit/master/content/c6/s1/boosting.md">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="boosting">
<h1>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\]</div>
<p>Like bagging and random forests, boosting combines multiple weak learners into one improved model. Unlike bagging and random forests, however, boosting trains these weak learners <em>sequentially</em>, each one learning from the mistakes of the last. Rather than a single model, “boosting” refers to a class of sequential learning methods.</p>
<p>Here we discuss two specific algorithms for conducting boosting. The first, known as <em>Discrete AdaBoost</em> (or just <em>AdaBoost</em>), is used for binary classification. The second, known as <em>AdaBoost.R2</em>, is used for regression.</p>
<div class="section" id="adaboost-for-binary-classification">
<h2>AdaBoost for Binary Classification<a class="headerlink" href="#adaboost-for-binary-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="weighted-classification-trees">
<h3>Weighted Classification Trees<a class="headerlink" href="#weighted-classification-trees" title="Permalink to this headline">¶</a></h3>
<p>Weak learners in a boosting model learn from the mistakes of previous iterations by increasing the <em>weights</em> of observations that previous learners struggled with. How exactly we fit a <em>weighted</em> learner depends on the type of learner. Fortunately, for classification trees this can be done with just a slight adjustment to the loss function.</p>
<p>Consider a  classification tree with classes <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span> and a node <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span>. Let  <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> give the fraction of the observations in <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> belonging to class <span class="math notranslate nohighlight">\(k\)</span>. Recall that the Gini index is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{G}(\mathcal{N}_m) = \sum_{k = 1}^K \hat{p}_{mk}(1-\hat{p}_{mk}),
\]</div>
<p>and the cross entropy as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{E}(\mathcal{N}_m) = -\sum_{k = 1}^K \hat{p}_{mk} \log\hat{p}_{mk},
\]</div>
<p>The classification tree then chooses the split <span class="math notranslate nohighlight">\(S_m\)</span> that minimizes the combined loss of the child nodes,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(S_m) = f_L\cdot \mathcal{L}(\mathcal{C}_m^L) +  f_R\cdot \mathcal{L}(\mathcal{C}_m^R),
\]</div>
<p>where <span class="math notranslate nohighlight">\(C_m^L\)</span> and <span class="math notranslate nohighlight">\(C_m^R\)</span> are the left and right child nodes and <span class="math notranslate nohighlight">\(f_L\)</span> and <span class="math notranslate nohighlight">\(f_R\)</span> are the fraction of observations going to each.</p>
<p>To allow for observation weighting, we simply change our definition of <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> to the following:</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_{mk} = \frac{\sum_{n \in \mathcal{N}_m} w_n I(y_n = k)}{\sum_{n \in \mathcal{N}_m} w_n }.
\]</div>
<p>This forces the tree to prioritize isolating the classes that were poorly classified by previous learners. The Gini index and cross-entropy are then defined as before and the rest of the tree is fit in the same way.</p>
</div>
<div class="section" id="the-discrete-adaboost-algorithm">
<h3>The Discrete AdaBoost Algorithm<a class="headerlink" href="#the-discrete-adaboost-algorithm" title="Permalink to this headline">¶</a></h3>
<p>The <em>Discrete AdaBoost</em> algorithm is outlined below. We start by setting the weights equal for each observation. Then we build <span class="math notranslate nohighlight">\(T\)</span> weighted weak learners (decision trees in our case). Each observation’s weight is updated according to two factors: the strength of the iteration’s learner and the accuracy of the learner on that observation. Specifically, the update for the weight on observation <span class="math notranslate nohighlight">\(n\)</span> at iteration <span class="math notranslate nohighlight">\(t\)</span> takes the form</p>
<div class="math notranslate nohighlight">
\[
w^{t+1}_n = w_n\exp(\alpha^t I^t_n),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha^t\)</span> is a measure of the accuracy of the learner and <span class="math notranslate nohighlight">\(I^t_n\)</span> indicates the learner misclassified observation <span class="math notranslate nohighlight">\(n\)</span>. This implies that the weight for a correctly-classified observation does not change while the weight for a misclassified observation increases, particularly if the model is accurate.</p>
<div class="alert alert-info">
<p class="admonition-title">Note</p>
<p>Notation for boosting can get confusing since we iterate over learners and through observations. For the rest of this page, let superscripts refer to the model iteration and subscripts refer to the observation. For instance, <span class="math notranslate nohighlight">\(w^t_n\)</span> refers to the weight of observation <span class="math notranslate nohighlight">\(n\)</span> in learner <span class="math notranslate nohighlight">\(t\)</span>.</p>
</div>
<hr class="docutils" />
<p><strong>Discrete AdaBoost Algorithm</strong></p>
<p>Define the target variable to be <span class="math notranslate nohighlight">\(y_n \in \{-1, +1 \}\)</span>.</p>
<ol>
<li><p>Initialize the weights with <span class="math notranslate nohighlight">\(w^1_n = \frac{1}{N}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 1, \dots, T\)</span>,</p>
<ul>
<li><p>Build weak learner <span class="math notranslate nohighlight">\(t\)</span> using weights <span class="math notranslate nohighlight">\(\mathbf{w}^t\)</span>.</p></li>
<li><p>Use weak learner <span class="math notranslate nohighlight">\(t\)</span> to calculate fitted values <span class="math notranslate nohighlight">\(f^t(\bx_n) \in \{-1, +1\}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>. Let <span class="math notranslate nohighlight">\(I^t_n\)</span> equal 1 If <span class="math notranslate nohighlight">\(f^t(\bx_n) \neq y_n\)</span> and 0 otherwise. That is, <span class="math notranslate nohighlight">\(I^t_n\)</span> indicates whether learner <span class="math notranslate nohighlight">\(t\)</span> misclassifies observation <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p>Calculate the weighted error for learner <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \epsilon^t = \frac{\sumN w^t_n I^t_n}{\sumN w^t_n}.
      \]</div>
</li>
<li><p>Calculate the accuracy measure for learner <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \alpha^t = \log\left(\frac{1-\epsilon^t}{\epsilon^t}\right).
      \]</div>
</li>
<li><p>Update the weighting with</p>
<div class="math notranslate nohighlight">
\[
      w^{t + 1}_n = w^t_n\exp(\alpha^tI^t_n),
      \]</div>
<p>for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p>
</li>
</ul>
</li>
<li><p>Calculate the overall fitted values with <span class="math notranslate nohighlight">\(\hat{y}_n = \text{sign} \left( \sum_{t = 1}^T \alpha^t f^t(\bx_n)\right)\)</span>.</p></li>
</ol>
<hr class="docutils" />
<p>Note that our final fitted value for observation <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\hat{y}_n\)</span>, is a weighted vote of all the individual fitted values where higher-performing learners are given more weight.</p>
</div>
</div>
<div class="section" id="adaboost-for-regression">
<h2>AdaBoost For Regression<a class="headerlink" href="#adaboost-for-regression" title="Permalink to this headline">¶</a></h2>
<p>We can apply the same principle of learning from our mistakes to regression tasks as well. A common algorithm for doing so is <em>AdaBoost.R2</em>, shown below. Like <em>AdaBoost</em>, this algorithm uses weights to emphasize observations that previous learners struggled with. Unlike <em>AdaBoost</em>, however, it does not incorporate these weights into the loss function directly. Instead, every iteration, it draws bootstrap samples from the training data where observations with greater weights are more likely to be drawn.</p>
<p>We then fit a weak learner to the bootstrapped sample, calculate the fitted values on the <em>original sample</em> (i.e. <em>not</em> the bootstrapped sample), and use the residuals to assess the quality of the weak learner. As in <em>AdaBoost</em>, we update the weights each iteration based on the quality of the learner (determined by <span class="math notranslate nohighlight">\(\beta^t\)</span>) and the accuracy of the learner on the observation (determined by <span class="math notranslate nohighlight">\(L_n\)</span>).</p>
<p>After iterating through many weak learners, we form our fitted values. Specifically, for each observation we use the weighted median (defined below) of the weak learners’ predictions, weighted by <span class="math notranslate nohighlight">\(\log(1/\beta^t)\)</span> for <span class="math notranslate nohighlight">\(t = 1, \dots, T\)</span>.</p>
<div class="alert alert-info">
<p class="admonition-title">Note</p>
<p><strong>Weighted Median</strong></p>
<p>Consider a set of values <span class="math notranslate nohighlight">\(\{v_1, v_2, \dots, v_N\}\)</span> and a set of corresponding weights <span class="math notranslate nohighlight">\(\{s_1, s_2, \dots, s_N\}\)</span>. Normalize the weights so they add to 1. To calculate the weighted median, sort the values and weights in ascending order of the values; call these <span class="math notranslate nohighlight">\(\{v^{(1)}, v^{(2)}, \dots, v^{(N)}\}\)</span> and  <span class="math notranslate nohighlight">\(\{s^{(1)}, s^{(2)}, \dots, s^{(N)}\}\)</span> where, for instance, <span class="math notranslate nohighlight">\(v^{(1)}\)</span> is the smallest value and <span class="math notranslate nohighlight">\(s^{(1)}\)</span> is its corresponding weight. The weighted median is then the value corresponding to the first weight such that the cumulative sum of the weights is greater than or equal to 0.5.</p>
<p>As an example, consider the values <span class="math notranslate nohighlight">\(\mathbf{v} = \{10,30,20,40\}\)</span> and corresponding weights <span class="math notranslate nohighlight">\(\mathbf{s} = \{0.4, 0.6, 0.2, 0.8\}\)</span>. First normalize the weights to sum to 1, giving <span class="math notranslate nohighlight">\(\mathbf{s} = \{0.2, 0.3, 0.1, 0.4\}\)</span>. Then sort the values and the weights, giving <span class="math notranslate nohighlight">\(\mathbf{v} = \{10, 20, 30, 40\}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{s} = \{0.2, 0.1, 0.3, 0.4\}\)</span>. Then, since the third element is the first for which the cumulative sum of the weights is at least 0.5, the weighted median is the third of the sorted values, <span class="math notranslate nohighlight">\(30\)</span>.</p>
</div>
<p>Though the arithmetic below gets somewhat messy, the concept is simple: iteratively fit a weak learner, see where the learner struggles, and emphasize the observations where it failed (where the amount of emphasis depends on the overall strength of the learner).</p>
<hr class="docutils" />
<p><strong>AdaBoost.R2 Algorithm</strong></p>
<ol>
<li><p>Initialize the weights with <span class="math notranslate nohighlight">\(w^1_n = \frac{1}{N}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 1, 2, \dots, T\)</span> or while <span class="math notranslate nohighlight">\(\bar{L}^t\)</span>, as defined below, is less than or equal to 0.5,</p>
<ul class="simple">
<li><p>Draw a sample of size <span class="math notranslate nohighlight">\(N\)</span> from the training data with replacement and with probability <span class="math notranslate nohighlight">\(w^t_n\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>Fit weak learner <span class="math notranslate nohighlight">\(t\)</span> to the resampled data and calculate the fitted values on the original dataset. Denote these fitted values with <span class="math notranslate nohighlight">\(f^t(\bx_{n})\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>.</p></li>
<li><p>Calculate the observation error <span class="math notranslate nohighlight">\(L^t_{n}\)</span> for <span class="math notranslate nohighlight">\(n = 1, 2, \dots, N\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    D^t &amp;= \underset{n}{\text{max}} \{ |y_{n} - f^t(\bx_{n})|  \} \\
    L^t_{n} &amp;= \frac{|y_{n} - f^t(\bx_{n})|}{D^t}
    \end{aligned}
    \end{split}\]</div>
<ul>
<li><p>Calculate the model error <span class="math notranslate nohighlight">\(\bar{L}^t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
      \bar{L}^t = \sum_{n = 1}^N  L^t_n w^t_n
      \]</div>
<p>If <span class="math notranslate nohighlight">\(\bar{L}^t \geq 0.5\)</span>, end iteration and set <span class="math notranslate nohighlight">\(T\)</span> equal to <span class="math notranslate nohighlight">\(t - 1\)</span>.</p>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(\beta^t = \frac{\bar{L}^t}{1- \bar{L}^t}\)</span>. The lower <span class="math notranslate nohighlight">\(\beta^t\)</span>, the greater our confidence in the model.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(Z^t = \sum_{n = 1}^N w^t_n (\beta^t)^{1 - L^t_n}\)</span> be a normalizing constant and update the model weights with</p>
<div class="math notranslate nohighlight">
\[
      w^{t + 1}_n = \frac{w^t_n (\beta^t)^{1 - L^t_n}}{Z^t},
      \]</div>
<p>which increases the weight for observations with a greater error <span class="math notranslate nohighlight">\(L^t_n\)</span>.</p>
</li>
</ul>
</li>
<li><p>Set the overall fitted value for observation <span class="math notranslate nohighlight">\(n\)</span> equal to the weighted median of <span class="math notranslate nohighlight">\(f^t(\bx_n)\)</span> for <span class="math notranslate nohighlight">\(t = 1, 2, \dots, T\)</span> using weights <span class="math notranslate nohighlight">\(\log(1/\beta^t)\)</span> for model <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ol>
<hr class="docutils" />
<p>In the boosting <a class="reference internal" href="../s2/boosting.html"><span class="doc">construction</span></a>, we implement <em>AdaBoost.R2</em> using decision tree regressors though many other weak learners may be used.</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="random_forests.html" title="previous page">Random Forests</a>
    <a class='right-next' id="next-link" href="../construction.html" title="next page">Construction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
  </body>
</html>