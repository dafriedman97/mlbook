

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Classification Trees &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/mystnb.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}})</script>
    <link rel="shortcut icon" href="../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Construction" href="../construction.html" />
    <link rel="prev" title="Regression Trees" href="regression_tree.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../../table_of_contents.html">Table of Contents</a>
  </li>
  <li class="">
    <a href="../../conventions_notation.html">Conventions and Notation</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">1. Ordinary Linear Regression</p>
</li>
  <li class="">
    <a href="../../c1/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c1/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c1/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">2. Linear Regression Extensions</p>
</li>
  <li class="">
    <a href="../../c2/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c2/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c2/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">3. Discriminative Classifiers (Logistic Regression)</p>
</li>
  <li class="">
    <a href="../../c3/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c3/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c3/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">4. Generative Classifiers (Naive Bayes)</p>
</li>
  <li class="">
    <a href="../../c4/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c4/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c4/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">5. Decision Trees</p>
</li>
  <li class="active">
    <a href="../concept.html">Concept</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="regression_tree.html">Regression Trees</a>
    </li>
    <li class="active">
      <a href="">Classification Trees</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">6. Tree Ensemble Methods</p>
</li>
  <li class="">
    <a href="../../c6/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c6/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c6/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">7. Neural Networks</p>
</li>
  <li class="">
    <a href="../../c7/concept.html">Concept</a>
  </li>
  <li class="">
    <a href="../../c7/construction.html">Construction</a>
  </li>
  <li class="">
    <a href="../../c7/code.html">Code</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Appendix</p>
</li>
  <li class="">
    <a href="../../appendix/math.html">Math</a>
  </li>
  <li class="">
    <a href="../../appendix/probability.html">Probability</a>
  </li>
  <li class="">
    <a href="../../appendix/methods.html">Common Methods</a>
  </li>
  <li class="">
    <a href="../../appendix/data.html">Datasets</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../../../_sources/content/c5/s1/classification_tree.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        <a class="edit-button" href="https://github.com/dafriedman97/book/edit/master/content/c5/s1/classification_tree.md"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" title="Edit this page"><i class="fas fa-pencil-alt"></i></button></a>

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#building-a-tree" class="nav-link">Building a Tree</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#the-objective" class="nav-link">The Objective</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#making-splits" class="nav-link">Making Splits</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#making-predictions" class="nav-link">Making Predictions</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#choosing-hyperparameters" class="nav-link">Choosing Hyperparameters</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#size-regulation" class="nav-link">Size Regulation</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#maximum-split-loss" class="nav-link">Maximum Split Loss</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#pruning" class="nav-link">Pruning</a>
        </li>
    
            </ul>
        </li>
    
    </ul>
</nav>



<div class="tocsection editthispage">
    <a href="https://github.com/dafriedman97/book/edit/master/content/c5/s1/classification_tree.md">
        <i class="fas fa-pencil-alt"></i> Edit this page
    </a>
</div>

    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="classification-trees">
<h1>Classification Trees<a class="headerlink" href="#classification-trees" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\]</div>
<p>Building a classification tree is essentially identical to building a regression tree but optimizing a different loss function—one fitting for a categorical target variable. For that reason, this section only covers the details unique to classification trees, rather than demonstrating how one is built from scratch. To understand the tree-building process in general, see the <a class="reference internal" href="regression_tree.html"><span class="doc"> previous section</span></a>.</p>
<p>Suppose for the following that we have data <span class="math notranslate nohighlight">\(\{\bx_n, y_n\}_{n = 1}^N\)</span> with predictor variables <span class="math notranslate nohighlight">\(\bx_n \in \R^D\)</span> and a categorical target variable <span class="math notranslate nohighlight">\(y_n \in \{1, \dots, K\}\)</span> .</p>
<div class="section" id="building-a-tree">
<h2>Building a Tree<a class="headerlink" href="#building-a-tree" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-objective">
<h3>The Objective<a class="headerlink" href="#the-objective" title="Permalink to this headline">¶</a></h3>
<p>Two common loss functions for a classification are the <em>Gini index</em> and the <em>cross-entropy</em>. Let <span class="math notranslate nohighlight">\(n \in \mathcal{N}_m\)</span> be the collection of training observations that pass through node <span class="math notranslate nohighlight">\(m\)</span> and let <span class="math notranslate nohighlight">\(\hat{y}_{mk}\)</span> be the fraction of these observations in class <span class="math notranslate nohighlight">\(k\)</span> for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>. The Gini index for <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{G}(\mathcal{N}_m) = \sum_{k = 1}^K \hat{p}_{mk}(1-\hat{p}_{mk}),
\]</div>
<p>and the cross-entropy is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{E}(\mathcal{N}_m) = -\sum_{k = 1}^K \hat{p}_{mk} \log\hat{p}_{mk}.
\]</div>
<p>The Gini index and cross-entropy are measures of <em>impurity</em>—they are higher for nodes with more equal representation of different classes and lower for nodes represented largely by a single class. As a node becomes more pure, these loss measures tend toward zero.</p>
<p>In order to evaluate the purity of a <em>split</em> (rather than that of a <em>node</em>), we use the weighted Gini index or weighted cross-entropy. Consider a split <span class="math notranslate nohighlight">\(S_m\)</span> of bud <span class="math notranslate nohighlight">\(\mathcal{N}_m\)</span> which creates children <span class="math notranslate nohighlight">\(\mathcal{C}_m^L\)</span> and <span class="math notranslate nohighlight">\(\mathcal{C}_m^R\)</span>. Let the fraction of training observations going to <span class="math notranslate nohighlight">\(\mathcal{C}_m^L\)</span> be <span class="math notranslate nohighlight">\(f_L\)</span> and the fraction going to <span class="math notranslate nohighlight">\(\mathcal{C}_m^R\)</span> be <span class="math notranslate nohighlight">\(f_R\)</span>. The weighted loss (whether with the Gini index or the cross-entropy) is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(S_m) = f_L\cdot \mathcal{L}(\mathcal{C}_m^L) +  f_R\cdot \mathcal{L}(\mathcal{C}_m^R).
\]</div>
<p>The lower the weighted loss the better.</p>
</div>
<div class="section" id="making-splits">
<h3>Making Splits<a class="headerlink" href="#making-splits" title="Permalink to this headline">¶</a></h3>
<p>As with regression trees, we will make splits one layer at a time. When splitting bud <span class="math notranslate nohighlight">\(m\)</span>, we use the same procedure as in regression trees: we calculate the loss from splitting the node at each value of each predictor and make the split with the lowest loss.</p>
<p>For quantitative predictors, the procedure is identical to the regression tree procedure except we aim to minimize <span class="math notranslate nohighlight">\(\mathcal{L}(S_m)\)</span> rather than maximally reducing <span class="math notranslate nohighlight">\(RSS_m\)</span>. For categorical predictors, we cannot rank the categories according to the average value of the target variable as we did for regression trees because the target is not continuous! If our target is binary, we can rank the predictor’s categories according to the fraction of the corresponding target variables in class <span class="math notranslate nohighlight">\(1\)</span> versus class <span class="math notranslate nohighlight">\(0\)</span> and proceed in the same was as we did for regression trees.</p>
<p>If the target is not binary, we are out of luck. One (potentially computationally-intensive) method is to try all possible binary groupings of the categorical value and see what grouping minimizes <span class="math notranslate nohighlight">\(\mathcal{L}(S_m)\)</span>. Another would be a one-versus-rest approach where we only consider isolating one category at a time from the rest. Suppose we had a predictor with four categories, <span class="math notranslate nohighlight">\(A, B, C,\)</span> and <span class="math notranslate nohighlight">\(D\)</span>. The first method requires the following 7 splits while the second method requires only the first four splits.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
A &amp;\text{ vs. } B, C, D \\
B &amp;\text{ vs. } A, C, D \\
C &amp;\text{ vs. } A, B, D \\
D &amp;\text{ vs. } A, C, D\\
A, B &amp;\text{ vs. }C, D \\
A, C &amp;\text{ vs. }B, D \\
A, D &amp;\text{ vs. }B, C \\
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="making-predictions">
<h3>Making Predictions<a class="headerlink" href="#making-predictions" title="Permalink to this headline">¶</a></h3>
<p>Classifying test observations with a fully-grown tree is very straightforward. First, run an observation through the tree and observe which leaf it lands in. Then classify it according to the most common class in that leaf.</p>
<p>For large enough leaves, we can also estimate the probability that the test observation belongs to any given class: if test observation <span class="math notranslate nohighlight">\(j\)</span> lands in leaf <span class="math notranslate nohighlight">\(m\)</span>, we can estimate <span class="math notranslate nohighlight">\(p(y_j =  k)\)</span> with <span class="math notranslate nohighlight">\(\hat{p}_{mk}\)</span> for each <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
</div>
<div class="section" id="choosing-hyperparameters">
<h2>Choosing Hyperparameters<a class="headerlink" href="#choosing-hyperparameters" title="Permalink to this headline">¶</a></h2>
<p>In the regression tree section, we discussed three methods for managing a tree’s size to balance the bias-variance tradeoff. The same three methods can be used for classification trees with slight modifications, which we cover next. For a full overview on these methods, please review the regression tree section.</p>
<div class="section" id="size-regulation">
<h3>Size Regulation<a class="headerlink" href="#size-regulation" title="Permalink to this headline">¶</a></h3>
<p>We can again use cross validation to fix the maximum depth of a tree or the minimum size of its terminal nodes. Unlike with regression trees, however, it is common to use a different loss function for cross validation than we do for building the tree. Specifically, we typically build classification trees with the Gini index or cross-entropy but use the <em>misclassification rate</em> to determine the hyperparameters with cross validation. The <em>misclassification rate</em> is simply the percent of observations we incorrectly classify. This is typically a more desirable metric to minimize than the Gini index or cross-entropy since it tells us more about our ultimate goal of correctly classifying test observations.</p>
<p>To conduct cross validation, then, we would build the tree using the Gini index or cross-entropy for a set of hyperparameters, then pick the tree with the lowest misclassification rate on validation samples.</p>
</div>
<div class="section" id="maximum-split-loss">
<h3>Maximum Split Loss<a class="headerlink" href="#maximum-split-loss" title="Permalink to this headline">¶</a></h3>
<p>Another regularization method for regression trees was to require that each split reduce the <span class="math notranslate nohighlight">\(RSS\)</span> by a certain amount. An equivalent approach for classification trees is to require that each split have a weighted loss below some minimum threshold. This threshold should be chosen through cross validation, again likely with the misclassification rate as the loss function.</p>
</div>
<div class="section" id="pruning">
<h3>Pruning<a class="headerlink" href="#pruning" title="Permalink to this headline">¶</a></h3>
<p>To prune a regression tree, we first fit a large tree, then searched for a sub-tree that could achieve a low <span class="math notranslate nohighlight">\(RSS\)</span> without growing too large and possibly overfitting. Specifically, we looked for the sub-tree <span class="math notranslate nohighlight">\(T\)</span> that minimized the following loss function, where <span class="math notranslate nohighlight">\(|T|\)</span> gives the number of terminal leaves and <span class="math notranslate nohighlight">\(\lambda\)</span> is a regularization parameter:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(T) = RSS_T + \lambda|T|.
\]</div>
<p>We prune a classification tree in a nearly identical fashion. First, we grow an intentionally overfit tree, <span class="math notranslate nohighlight">\(T_0\)</span>. We then consider all splits leading to terminal nodes and undo the one with the greatest loss by re-joining its child nodes. I.e. we undo the split at the node <span class="math notranslate nohighlight">\(m\)</span> with the greatest <span class="math notranslate nohighlight">\(\mathcal{L}(S_m)\)</span> among nodes leading to leaves. We then repeat this process iteratively until we are left with the tree containing only the initial node. For each tree, we record its size (the number of terminal leaves) and its misclassification rate, <span class="math notranslate nohighlight">\(\mathcal{M}_T\)</span>. We then choose the tree that minimizes</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(T) = \mathcal{M}_T + \lambda|T|, 
\]</div>
<p>where again <span class="math notranslate nohighlight">\(\lambda\)</span> is chosen through cross validation. For a fuller overview of how we use cross validation to choose <span class="math notranslate nohighlight">\(\lambda\)</span>, see the pruning section in the <a class="reference internal" href="regression_tree.html"><span class="doc">regression tree</span></a> page.</p>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="regression_tree.html" title="previous page">Regression Trees</a>
    <a class='right-next' id="next-link" href="../construction.html" title="next page">Construction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
  </body>
</html>