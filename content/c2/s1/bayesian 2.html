

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Bayesian Regression &#8212; Machine Learning from Scratch</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"sumN": "\\sum_{n = 1}^N", "sumn": "\\sum_{n}", "prodN": "\\prod_{n = 1}^N", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bT": "\\mathbf{T}", "bbeta": "\\boldsymbol{\\beta}", "btheta": "\\boldsymbol{\\hat{\\theta}}}", "bmu": "\\boldsymbol{\\mu}", "bSigma": "\\boldsymbol{\\Sigma}", "bbetahat": "\\boldsymbol{\\hat{\\beta}}", "bbR": "\\mathbb{R}", "iid": "\\overset{\\small{\\text{i.i.d.}}}{\\sim}}", "dadb": ["{\\frac{\\partial #1}{\\partial #2}}", 2], "testing": "\\TeX", "R": "\\mathbb{R}"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="GLMs" href="GLMs.html" />
    <link rel="prev" title="Regularized Regression" href="regularized.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning from Scratch</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../table_of_contents.html">
   Table of Contents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../conventions_notation.html">
   Conventions and Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  1. Ordinary Linear Regression
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c1/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c1/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c1/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  2. Linear Regression Extensions
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="../concept.html">
   Concept
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="regularized.html">
     Regularized Regression
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Bayesian Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="GLMs.html">
     GLMs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  3. Discriminative Classifiers (Logistic Regression)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c3/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c3/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c3/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  4. Generative Classifiers (Naive Bayes)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c4/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c4/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c4/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  5. Decision Trees
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c5/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c5/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c5/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  6. Tree Ensemble Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c6/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c6/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c6/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  7. Neural Networks
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../c7/concept.html">
   Concept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c7/construction.html">
   Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../c7/code.html">
   Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/math.html">
   Math
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/probability.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/methods.html">
   Common Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../appendix/data.html">
   Datasets
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/c2/s1/bayesian.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/dafriedman97/mlbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/dafriedman97/mlbook/issues/new?title=Issue%20on%20page%20%2Fcontent/c2/s1/bayesian.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/dafriedman97/mlbook/edit/master/content/c2/s1/bayesian.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-bayesian-structure">
   The Bayesian Structure
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-likelihood">
     1. The Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-prior">
     2. The Prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-posterior">
     3. The Posterior
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results">
   Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition">
     Intuition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#full-results">
     Full Results
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayesian-regression">
<h1>Bayesian Regression<a class="headerlink" href="#bayesian-regression" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[
\newcommand{\sumN}{\sum_{n = 1}^N}
\newcommand{\sumn}{\sum_n}
\newcommand{\prodN}{\prod_{n = 1}^N}
\newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
\newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}}
\]</div>
<p>In the Bayesian approach to statistical inference, we treat our parameters as random variables and assign them a prior distribution. This forces our estimates to reconcile our existing beliefs about these parameters with new information given by the data. This approach can be applied to linear regression by assigning the regression coefficients a prior distribution.</p>
<p>We also may wish to perform Bayesian regression not because of a prior belief about the coefficients but in order to minimize model complexity. By assigning the parameters a prior distribution with mean 0, we force the posterior estimates to be closer to 0 than they would otherwise. This is a form of regularization similar to the Ridge and Lasso methods discussed in the <a class="reference internal" href="regularized.html"><span class="doc">previous section</span></a>.</p>
<div class="section" id="the-bayesian-structure">
<h2>The Bayesian Structure<a class="headerlink" href="#the-bayesian-structure" title="Permalink to this headline">¶</a></h2>
<p>To demonstrate Bayesian regression, we’ll follow three typical steps to Bayesian analysis: writing the likelihood, writing the prior density, and using Bayes’ Rule to get the posterior density. In the <a class="reference internal" href="#results"><span class="std std-ref">results</span></a> below, we use the posterior density to calculate the maximum-a-posteriori (MAP)—the equivalent of calculating the <span class="math notranslate nohighlight">\(\hat{\bbeta}\)</span> estimates in ordinary linear regression.</p>
<div class="section" id="the-likelihood">
<h3>1. The Likelihood<a class="headerlink" href="#the-likelihood" title="Permalink to this headline">¶</a></h3>
<p>As in the typical regression set-up, let’s assume</p>
<div class="math notranslate nohighlight">
\[
Y_n \iid \mathcal{N}\left(\bbeta^\top \bx_n , \sigma^2\right).
\]</div>
<p>We can write the collection of observations jointly as</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\by &amp;\sim \mathcal{N}\left( \bX\bbeta, \bSigma\right),
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\by \in \mathbb{R}^N\)</span> and <span class="math notranslate nohighlight">\(\bSigma = \sigma^2 I_N \in \mathbb{R}^{N \times N}\)</span> for some <em>known</em> scalar <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Note that <span class="math notranslate nohighlight">\(\by\)</span> is a vector of random variables—it is not capitalized in order to distinguish it from a matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://www.statlect.com/fundamentals-of-statistics/Bayesian-regression">this lecture</a> for an example of Bayesian regression without the assumption of known variance.</p>
</div>
<p>We can then get our likelihood and log-likelihood using the Multivariate Normal.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L(\bbeta; \bX, \by) 
&amp;= 
\frac{1}{\sqrt{(2\pi)^N|\bSigma|}}\exp\left(-\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) \right) 
\\
&amp;\propto \exp\left(-\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) \right) 
\\
\log L(\bbeta; \bX, \by) &amp;= -\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta).
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="the-prior">
<h3>2. The Prior<a class="headerlink" href="#the-prior" title="Permalink to this headline">¶</a></h3>
<p>Now, let’s assign <span class="math notranslate nohighlight">\(\bbeta\)</span> a prior distribution. We typically assume</p>
<div class="math notranslate nohighlight">
\[
\bbeta \sim \mathcal{N}(\mathbf{0}, \bT),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bbeta \in \mathbb{R}^D\)</span> and <span class="math notranslate nohighlight">\(\bT = \tau I_D \in \mathbb{R}^{D \times D}\)</span> for some scalar <span class="math notranslate nohighlight">\(\tau\)</span>. We choose <span class="math notranslate nohighlight">\(\tau\)</span> (and therefore <span class="math notranslate nohighlight">\(\bT\)</span>) ourselves, with a greater <span class="math notranslate nohighlight">\(\tau\)</span> giving less weight to the prior.</p>
<p>The prior density is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\bbeta) &amp;= 
\frac{1}{\sqrt{(2\pi)^D|\bT|}}\exp\left(-\frac{1}{2}\bbeta^\top\bT^{-1}\bbeta \right) 
\\
&amp;\propto \exp\left(-\frac{1}{2}\bbeta^\top\bT^{-1}\bbeta \right)
\\
\log p(\bbeta) &amp;= -\frac{1}{2}\bbeta^\top \bT^{-1}\bbeta.
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="the-posterior">
<h3>3. The Posterior<a class="headerlink" href="#the-posterior" title="Permalink to this headline">¶</a></h3>
<p>We are then interested in a posterior density of <span class="math notranslate nohighlight">\(\bbeta\)</span> given the data, <span class="math notranslate nohighlight">\(\bX\)</span> and <span class="math notranslate nohighlight">\(\by\)</span>.</p>
<p>Bayes’ rule tells us that the posterior density of the coefficients is proportional to the likelihood of the data times the prior density of the coefficients. Using the two previous results, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\bbeta|\bX, \by) &amp;\propto L(\bbeta; \bX, \by) p(\bbeta) 
\\
\log p(\bbeta|\bX, \by) &amp;= \log L(\bbeta; \bX, \by) + \log p(\bbeta) + k
\\
&amp;=  -\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) - \frac{1}{2}\bbeta^\top \bT^{-1}\bbeta + k 
\\
&amp;= -\frac{1}{2\sigma^2}(\by - \bX\bbeta)^\top(\by - \bX\bbeta) - \frac{1}{2\tau}\bbeta^\top \bbeta + k 
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is some constant that we don’t care about.</p>
</div>
</div>
<div class="section" id="results">
<span id="id1"></span><h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<div class="section" id="intuition">
<h3>Intuition<a class="headerlink" href="#intuition" title="Permalink to this headline">¶</a></h3>
<p>Often in the Bayesian setting it is infeasible to obtain the entire posterior distribution. Instead, one typically looks at the maximum-a-posteriori (MAP), the value of the parameters that maximize the posterior density. In our case, the MAP is the <span class="math notranslate nohighlight">\(\bbetahat\)</span> that maximizes</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\log p(\bbetahat|\bX, \by) &amp;= -\frac{1}{2\sigma^2}(\by - \bX\bbetahat)^\top(\by - \bX\bbetahat) - \frac{1}{2\tau}\bbetahat^\top \bbetahat.
\end{align*}
\]</div>
<p>This is equivalent to finding the <span class="math notranslate nohighlight">\(\bbetahat\)</span> that minimizes the following loss function, where <span class="math notranslate nohighlight">\(\lambda = 1/\tau\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
L(\bbetahat) &amp;= \frac{1}{2}(\by - \bX\bbetahat)^\top(\by - \bX\bbetahat) + \frac{\lambda}{2}\bbetahat^\top \bbetahat 
\\
&amp;= \frac{1}{2}(\by - \bX\bbetahat)^\top(\by - \bX\bbetahat) + \frac{\lambda}{2} \sum_{d = 0}^D\hat{\beta}_d.
\end{align}
\end{split}\]</div>
<p>Notice that this is extremely close to the Ridge loss function discussed in the <a class="reference internal" href="regularized.html"><span class="doc">previous section</span></a>—it is not quite equal to the Ridge loss function since it also penalizes the magnitude of the intercept, though this difference could be eliminated by changing the prior distribution of the intercept.</p>
<p>This shows that Bayesian regression with a mean-zero Normal prior distribution is essentially equivalent to Ridge regression. Decreasing <span class="math notranslate nohighlight">\(\tau\)</span>, just like increasing <span class="math notranslate nohighlight">\(\lambda\)</span>, increases the amount of regularization.</p>
</div>
<div class="section" id="full-results">
<h3>Full Results<a class="headerlink" href="#full-results" title="Permalink to this headline">¶</a></h3>
<p>Now let’s actually derive the MAP by calculating the gradient of the log posterior density.</p>
<div class="admonition-math-note admonition">
<p class="admonition-title">Math Note</p>
<p>For a symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \mathbf{s}}\left(\mathbf{q} - \mathbf{A}\mathbf{s} \right)^\top \mathbf{W}\left(\mathbf{q} - \mathbf{A}\mathbf{s}\right) = -2\mathbf{A}^\top \mathbf{W}\left(\mathbf{q} - \mathbf{A}\mathbf{s}\right)
\]</div>
<p>This implies that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \mathbf{s}}\mathbf{s}^\top \mathbf{W}\mathbf{s} = 
\frac{\partial}{\partial \mathbf{s}} (\mathbf{0} - I\mathbf{s})^\top \mathbf{W} (\mathbf{0} - I\mathbf{s})= 
2\mathbf{W}\mathbf{s}.
\]</div>
</div>
<p>Using the <em>Math Note</em> above, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log p(\bbetahat|\bX, \by) &amp;=  -\frac{1}{2}(\by - \bX\bbeta)^\top\bSigma^{-1}(\by - \bX\bbeta) - \frac{1}{2}\bbeta^\top \bT^{-1}\bbeta \\
\dadb{}{\bbeta} \log p(\bbeta|\bX, \by) &amp;= \bX^\top \bSigma^{-1}(\by - \bX \bbeta) - \bT^{-1}\bbeta.
\end{align*}
\end{split}\]</div>
<p>We calculate the MAP by setting this gradient equal to 0:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\bbetahat &amp;= \left(\bX^\top\bSigma^{-1} \bX + \bT^{-1}\right)^{-1}\bX^\top\bSigma^{-1}\by \\
&amp;= \left(\frac{1}{\sigma^2}\bX^\top\bX + \frac{1}{\tau} I\right)^{-1}\frac{1}{\sigma^2}\bX^\top\by.
\end{align*}
\end{split}\]</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/c2/s1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="regularized.html" title="previous page">Regularized Regression</a>
    <a class='right-next' id="next-link" href="GLMs.html" title="next page">GLMs</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Danny Friedman<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
  </body>
</html>